# 聚类

参考资料:  

> https://www.cnblogs.com/niniya/p/8784947.html
>
> 吴老师 聚类分析
> 可能还要参考其他的一些资料

聚类指的是把集合，分组成多个类，每个类中的对象都是彼此相似的。

目标: 把聚类所需要的知识都整理到这一片文档中来, 如果内容过长, 考虑后面进行拆分

### 聚类的目的:

聚类分析是无监督机器学习算法中最常用的一类, 目的是将数据划分成有意义或者有用的组.(术语称为 簇 ).

如果结果是有意义的, 则簇应当捕获数据的自然结构.

#### 1. 旨在理解数据的自然结构

人类擅长将对象划分成组(聚类 ),并将特定的对象指派到这些组( 分类 ),

就理解数据而言, 簇是潜在的类, 而聚类分析是试图通过算法, 自动的发现这些类.

1.  生物学
2. 信息检索
3. 气候
4. 心理学和医学
5. 商业

#### 2. 用于处理数据的聚类

聚类分析的一个应用是,  它能够提供由个别数据对象到数据对象所指派的簇的抽象. 一些聚类技术使用簇原型( 即同一个簇中用于代表其他对象的数据对象, 例如质心) 来刻画簇特征. 这些簇原型可以用作大量数据分析和数据处理技术的基础. 

因此, 就聚类用于数据处理层面而言, 聚类分析是研究发现最有代表性的簇原型的技术.

##### 聚类用于降维

许多数据分析技术, 如回归和PCA, 都具有O(m2) 或更高的时间或空间复杂度, 因此, 对于大型数据集, 这些技术无法应用. 然而, 可以将算法用于仅包含簇原型的数据集,  即每一列只保留其原型, 而不是整个数据集.

##### 聚类用于数据离散压缩

簇原型可以用于数据列压缩, 例如, 创建一个包含所有簇原型的表, 即每一个原型赋予一个整数值, 作为它在表中的位置( 索引 ) . 每个对象, 用与它所在的簇相关联的原型的索引表示.

这类压缩称为向量量化(vector quantization),  并常常用于图像,  声音和视频数据.

当然, 无论是使用数据降维还是数据压缩, 都将损失一定的数据信息量, 而能够使用聚类进行信息浓缩的数据特点是: (1) 许多数据对象之间高度相似,  (2)  某些信息丢失是可以接受的,  (3) 希望大幅度压缩数据量

> 感觉这样的写法, 确实信息浓缩程度过于低了, 并不怎么太好

##### 有效的发现最近邻

很多机器学习算法都是基于距离的模型, 即找出最近邻可能需要计算所有点对对之间的距离, 如之前介绍的KNN. 





## kmeans

K-means是聚类中最常用的方法之一，它是基于点与点距离的相似度来计算最佳类别归属。

### 在使用该方法前，要注意

1. 对数据异常值的处理；
2. 对数据标准化处理（x-min(x))/(max(x)-min(x)）；
3. 每一个类别的数量要大体均等；
4. 不同类别间的特质值应该差异较大

### 步骤:

1. 有一堆数据, 我们也没有什么类别标注, 我还想探索一下数据中的特点, 把相似的点分成一堆, 祈祷或许这个结果是有价值的.
2. 先确定一共要分成几堆( 最后不一定有那么多堆 ), 也就是确定第一个参数 **k**
3. 在数据集中随机的选择 k 个点, 作为我们聚类的初始位置.
4. 对每一个点分别计算, 到这几个中心点的距离, 算出离哪个最近, 就把它分到哪一类中去.
5. 依据划分好的类别, 重新计算几个质心.
6. 根据新的质心, 重复第4步, 第5步.
7. 直到中心不再发生变化了, 或者重复次数达到设定的值
8. 结束, 得到k个(或者不到k个聚类)

### 聚类结果评估:

指标: 

1. **inertias** 

   是K-Means模型对象的属性，它作为没有真实分类结果标签下的非监督式评估指标。表示样本到最近的聚类中心的距离总和。值越小越好，越小表示样本在类间的分布越集中。 

2. **兰德指数（Rand index）**

   需要给定实际类别信息C，假设K是聚类结果，a表示在C与K中都是同类别的元素对数，b表示在C与K中都是不同类别的元素对数，则兰德指数为： 

   

![img](pictures/115) 



$RI$ 取值范围为[0,1]，值越大意味着聚类结果与真实情况越吻合。 

对于随机结果，$RI$并不能保证分数接近零。为了实现“在聚类结果随机产生的情况下，指标应该接近零”，调整兰德系数（Adjusted rand index）被提出，它具有更高的区分度： 

![img](pictures/116) 

$ARI$取值范围为[−1,1]，值越大意味着聚类结果与真实情况越吻合。从广义的角度来讲，$ARI$衡量的是两个数据分布的吻合程度。 



3. **互信息（Mutual Information，MI）**

   指的是相同数据的两个标签之间的相似度，即也是在衡量两个数据分布的相似程度。利用互信息来衡量聚类效果需要知道实际类别信息。 

假设U与V是对N个样本标签的分配情况，则两种分布的熵分别为： 

![img](pictures/117) 



其中: 

![img](pictures/118) 



U与V之间的互信息（MI）定义为： 

![img](pictures/119) 

其中:

![img](pictures/120) 



标准化后的互信息（Normalized Mutual Information）： 

![img](pictures/121) 

调整互信息（Adjusted Mutual Information）： 

![img](pictures/122) 

MI与NMI取值范围[0,1]，AMI取值范围[-1,1]，都是值越大说明聚类效果越好。

4. **同质化得分（Homogeneity）**

   如果所有的聚类都只包含属于单个类的成员的数据点，则聚类结果满足同质性。取值范围[0,1]，值越大意味着聚类结果与真实情况越符合。

5. **完整性得分（Complenteness）**

   如果作为给定类的成员的所有数据点是相同集群的元素，则聚类结果满足完整性。取值范围[0,1]，值越大意味着聚类结果与真实情况越符合。

6. **v_meansure_score**

   同质化和完整性之间的谐波平均值，v=2*（同质化*完整性）/（同质化+完整性），取值范围[0,1]，值越大意味着聚类结果与真实情况越符合。

7. **轮廓系数（Silhouette）**

   适用于实际类别信息未知的情况，用来计算所有样本的平均轮廓系数。对于单个样本，设a是该样本与它同类别中其他样本的平均距离，b是与它距离最近不同类别中样本的平均距离，轮廓系数为：

   ![img](pictures/123) 

   对于一个样本集合，它的轮廓系数是所有样本轮廓系数的平均值，轮廓系数取值范围是[−1,1]，0附近的值表示重叠的聚类，负值通常表示样本被分配到错误的集群，分数越高，说明同类别样本间距离近，不同类别样本间距离远。 

8. **calinski-harabaz Index** 

   适用于实际类别信息未知的情况，为群内离散与簇间离散的比值，值越大聚类效果越好。 



### KMeans 主要参数

1. n_clusters : K值

2. init : 初始值选择方式 

   可选值: 'k-means++'（用均值）、'random'（随机）、an ndarray（指定一个数组），默认为'k-means++' 

3. n_init： 用不同的初始化质心运行算法的次数 

   由于K-Means是结果受初始值影响的局部最优的迭代算法，因此需要多跑几次以选择一个较好的聚类效果，默认是10，一般不需要改，即程序能够基于不同的随机初始中心点独立运行算法10次，并从中寻找SSE（簇内误差平方和）最小的作为最终模型。如果k值较大，则可以适当增大这个值。 

4. max_iter： 最大的迭代次数 

   一般如果是凸数据集的话可以不管这个值，如果数据集不是凸的，可能很难收敛，此时可以指定最大的迭代次数让算法可以及时退出循环。 

5. algorithm：算法 

   可选值：“auto”, “full” or “elkan”。"full"指K-Means算法， “elkan”指elkan K-Means算法。默认的"auto"则会根据数据值是否是稀疏的，来决定如何选择"full"和“elkan”。一般数据是稠密的，那么就是 “elkan”，否则就是"full"。一般来说建议直接用默认的"auto"。

    

### k值的选择方法

基于簇内误差平方和，使用肘方法确定簇的最佳数量，肘方法的基本理念就是找出聚类偏差骤增是的k值，通过画出不同k值对应的聚类偏差图，可以清楚看出。







