# 一. 算法原理

## 1. 算法特点

1. 既可以分类, 也可以回归, 叫做分类树和回归树.
2. 通过递归式切割寻找最佳分类标准, 进而最终形成规则
3. 使用简单, 初始是一种非常朴素的分类思想, 无需进行过多变量调整和数据预处理
4. 规则清晰, 模型本身可解释性强.
5. 应用广泛

## 2. 核心思想和基本概念

回答一个物体多个属性的问题, 就可以得到他的类标号.

这一系列问题可以组织成树的形式, 我们将这种具有分类指向性的树状结构称为决策树, 决策树是一种由结点和有向边组成的层次结构.

决策树的本质还是一种**图结构**

对决策树的结构进行一些定义:

- 根节点（rootnote）：最上面的结点
- 内部节点（internalnode）：有一条入边和两条以上出边
- 叶节点（leafnode）或终结点（terminalnode）：只有一条入边， 没有出边
- 父节点和子节点：一条有向边连接的两个节点中， 出边的结点被称为入边结点的父节点， 后者被称为子节点。

每个叶节点都被赋予一个类标号， 非终结点包含属性测试条条件， 用以区分不同属性的记录， 决策树的构造构成实际上就是对原油数据集， 选取一定的属性测试条件（attribute test condition），不断进行切分的过程.

 决策树是生成模型的算法，一旦构造了决策树， 对检验记录进行分类就相当容易了。

## 3. 构建决策树

对于决策树， 所有构建方法为属性的指数级， 某些决策树比其他决策树更准确， 但是对于指数级的规模， 找出全局最佳决策树的方法是不可行的。

因此我们构建出一些有效的算法， 在有限的时间内找出具有一定准确性的次最优解。

这些算法通常采用贪心策略， 在选择划分数据集的属性时， 采取一系列局部最优策略来构建决策树。

例如Hunt算法， 是Hunt等人提出的最早的决策树模型， 是许多决策树算法的基础， 例如ID3， C4.5， CART等。

### 3.1 ID3算法构建决策树

符号定义：

t	代表决策树的某结点

D~t~	是t结点所对应的数据集

y = {y1, y2, ...y~n~}	则代表所属类别的集合. 

对于任何决策树而言, 树本身要有决策效力, 就必须让叶节点纯度尽量高(指叶节点, 也就是最终结点尽量只有一个类).纯度越高的决策树分类效果越好.

设p(i|t)表示给定结点t中属于类i所占的比例, 有时我们直接用pi来表示.

选择最佳划分的度量通常是根据划分后, 子女结点不纯性的程度, 不纯的程度越低, 类分布就越倾斜. 类分布为(0, 1)的结点具有零不纯性, 而均衡分布的结点(0.5, 0.5)的结点具有最高的不纯性. 

对于结点不纯度的计算和表示方法因决策树模型而有异, 但不管不纯度的度量方法如何, 都是有误差率衍生而来, 计算公式如下:
$$
Classfication error(t) = 1 - max_{i=1} [ p(i|t)]
$$
可以看出, 误差率越低, 则纯度越高. 由此还可以衍生出其他两个常用指标, 

一个是ID3中, 信息增益(Information gain) 的计算方法可以用Entropy推导, 即熵, 公式如下:
$$
Entropy = - \sum_{i=0}^{c-1}p(i|t)log_2{p(i|t)}
$$
在计算时设定log~2~0 = 0

另一个是Gini系数(基尼指数), 主要用于CART决策树的纯度判定, 公式如下
$$
Gini = 1 - \sum_{i=0}^{c-1}[p(i|t)^2]
$$
三种方法本质相同, 在类分布均衡时达到最大值, 而当所有记录都属于同一个类时, 达到最小值

熵在0-1区间内分布, 而Gini指数和分类误差均在0-0.5区间内分布, 三个指数随某变量占比增加而变化的曲线如下:

![1527691694761](pictures/90)

因此, 我们可以知道, 决策树最终的优化目标是使叶节点的总不纯度最低, 即对应衡量不纯度的指标最低. ID3采用信息熵来衡量不纯度, 最优条件为叶节点的总信息熵最小.

全局最有树没有办法简单高效获得, 因此我们是通过优化条件设置, 最终在每一部都是局部最优的条件下, 逐步推导至尽可能全局最优的结果.

决策树生成最优解的条件: 在选取属性测试条件(attribute test condition)对某节点, 进行切分的时候, 尽可能选取使得该节点对应的子节点信息熵最小的特征进行切分.

换言之, 就是要求父节点和子节点信息熵之差要最大, 对于ID3而言, 二者之差就是信息增益, 即资讯获利Information gain.

但此时计算子节点的总信息熵不能简单加和, 而要在求和前进行修正.

总不纯度计算公式:
$$
I(child) = \sum_{j=1}^k \frac{N(v_j)}{N}I(v_j)
$$
父节点和子节点的不纯度下降可由下述公式进行计算
$$
\Delta = I (parent) - I(child)
$$
I()是给定结点的不纯性度量, N是父节点上的记录总数, k是属性值的个数, N(~j~)是与子节点v~j~相关联的记录个数. 

决策树归纳算法通常选择最大化增益$\Delta$的测试条件, 因为对所有测试条件来说, I(parent) 是一个不变的值, 所以最大化信息增益等价于最小化子节点的不纯性度量的加权平均值.

最后, 当选择熵作为公式的不纯性度量, 熵的差就是所谓的信息增益(Information gain) $\Delta info$



> 总的来说, 决策树模型仍然是典型的机器学习模型, 总目标是一个全局最优解, 即一整套合理的分类规则, 使得最终叶节点的纯度最高, 但全局最优解很难获得, 因此我们退而求其次, 考虑采取全局最优解来一步步推导结果, 推导的过程仍然采用迭代的方法, 即上一次的输出结果作为下次执行的基本条件, 同时利用一个约束条件来判断模型最终收敛.
>
> 即叶节点纯度为1(或适当放宽, ), 而局部最优条件, 对于ID3来讲就是信息增益最大.

### 3.2 ID3的局限性

ID3的局限主要源于局部最优条件, 即信息增益的计算方法

- 分支度越高(分类水平越多)的离散变量往往子节点的总信息熵会更少, 极限情况是取ID作为切分字段, 则为0
- 不能直接处理连续性变量, 需要先对连续变量离散化, 
- 对缺失值较敏感, 需要提前对缺剪枝失值进行处理.
- 没有剪枝的设置, 容易导致过拟合.





# 决策树在保险行业中的应用--章金龙

项目背景:

美国某保险公司，与merkle合作多年，有一款医疗险产品
主要向65岁及以上的人群推出，渠道是直邮，客户想知道
哪些人更倾向于购买该医疗险产品？

## 多分类问题如何转化

假设我们的目标变量有五种分类，如何转化？

- 一对多

将类别1看作正类，其余2,3,4,5看作负类(一对多)，这样拿一
个样本来，可以告诉你是不是属于类别1的，如果不属于，再
在2,3,4,5中继续寻找

- 一对一

从1-5中任意选取2种类别来分类，得到5*4/2=10中分类器，每
一个分类器只告诉你是第一类还是第二类，或者是第一类还是
第三类，统计所有分类器的票数，根据票数得到分类结果。

## 分类问题 VS  回归问题

- 分类问题：尝试从一个或多个连续、分类预测变量中预测一个分类变量。

- 回归问题：尝试从一个或多个连续、分类预测变量中预测一个连续变量。
- 分类和回归的区别在于输出变量的类型。
  - 定量输出称为回归，或者说是连续型变量预测；
  - 定性输出称为分类，或者说是离散型变量预测。

## 完整的分类过程

**包括三个步骤：学习模块，验证模块，测试模块。**

![1527778115108](pictures/96)

## 分类模型的性能评价指标

####  混淆矩阵 (Confusion Matrix)

| 真实情况 | 预测结果   |            |
| -------- | ---------- | ---------- |
|          | 正例       | 反例       |
| 正例     | TP(真正例) | FN(反假例) |
| 反例     | FP(假正例) | TN(真反例) |

重要: 二元混淆矩阵的TP,FP,TN,FN可以这样理解：

这两个字母, 都不是指代真实的正反例情况, 而是都指的是, 我们的判断正误情况.

第一个字母: Is your judgement right(true) or not(false)?

你判断的是对了还是错了?

第二个字母: What's your judgement about the sample?

你对样本的判断是什么?



- 错误率（Error Rate）
-  敏感性( Sensitivity, True Positive Rate）= 真正例/实际正例总数
-  特异性(Specificity, True Negative Rate) = 真反例/实际反例总数
-  ROC曲线：x轴为1-specificity，y轴为sensitivity
-  Lift曲线：x轴为depth，y轴为lift

我的理解: ROC曲线实际上是这样的意义,  当我不断减小阈值的时候, 我就会将更多的样本预测为真实例, True Positive Rate的比例就会不断增高.因为总正例个数是固定的, 真正例越多自然TPR越高.

但是在减小阈值的同时, FP也会同步变高, 最极端的情况, 把阈值设置为0, 则全部样本预测为真, 这时, TPR为1, 而1-TNR 也为0了, 则曲线毫无意义.

如果把阈值设置为1, 则, TPR为0, 1-TNR也为0, 也没有意义.

在不断调整阈值的过程中, 1-TNR和TPR将会同步上升, 这时如果TPR(y轴)的值, 上升的明显快过1-TNR(x轴), 那么就说明, 我们的预测效果很好.



## ROC曲线

ROC曲线指受试者工作特征曲线 / 接收器操作特性曲线(receiver operating characteristic curve), 是反映敏感性和特异性连续变量的综合指标,是用构图法揭示敏感性和特异性的相互关系，它通过将连续变量设定出多个不同的临界值，从而计算出一系列敏感性和特异性，再以敏感性为纵坐标、（1-特异性）为横坐标绘制成曲线，曲线下面积越大，诊断准确性越高。在ROC曲线上，最靠近坐标图左上方的点为敏感性和特异性均较高的临界值。 

### ROC曲线的例子

考虑一个二分问题，即将实例分成正类（positive）或负类（negative）。对一个二分问题来说，会出现四种情况。如果一个实例是正类并且也被 预测成正类，即为真正类（True positive）,如果实例是负类被预测成正类，称之为假正类（False positive）。相应地，如果实例是负类被预测成负类，称之为真负类（True negative）,正类被预测成负类则为假负类（false negative）。

列联表如下表所示，1代表正类，0代表负类。 

　　

|      |      | 预测                      |                           |                        |
| ---- | ---- | ------------------------- | ------------------------- | ---------------------- |
|      |      | 1                         | 0                         | 合计                   |
| 实际 | 1    | True Positive（TP）       | False Negative（FN）      | Actual Positive(TP+FN) |
|      | 0    | False Positive（FP)       | True Negative(TN)         | Actual Negative(FP+TN) |
| 合计 |      | Predicted Positive(TP+FP) | Predicted Negative(FN+TN) | TP+FP+FN+TN            |

 从列联表引入两个新名词。其一是真正类率(true positive rate ,TPR), 计算公式为TPR=TP/ (TP+ FN)，刻画的是分类器所识别出的 正实例占所有正实例的比例。另外一个是假正类率(false positive rate, FPR),计算公式为FPR= FP / (FP + TN)，计算的是分类器错认为正类的负实例占所有负实例的比例。还有一个真负类率（True Negative Rate，TNR），也称为specificity,计算公式为TNR=TN/ (FP+ TN) = 1-FPR。 

![img](pictures/97) 

其中，两列True matches和True non-match分别代表应该匹配上和不应该匹配上的

两行Pred matches和Pred non-match分别代表预测匹配上和预测不匹配上的

FPR = FP/(FP + TN) 负样本中的错判率（假警报率）

TPR = TP/(TP + FN) 判对样本中的正样本率（命中率）

ACC = (TP + TN) / (P+N) 判对准确率

在一个二分类模型中，对于所得到的连续结果，假设已确定一个阀值，比如说 0.6，大于这个值的实例划归为正类，小于这个值则划到负类中。如果减小阀值，减到0.5，固然能识别出更多的正类，也就是提高了识别出的正例占所有正例 的比类，即TPR,但同时也将更多的负实例当作了正实例，即提高了FPR。为了形象化这一变化，在此引入ROC，ROC曲线可以用于评价一个分类器。 

![img](pictures/98) 

ROC曲线和它相关的比率

(a)理想情况下，TPR应该接近1，FPR应该接近0。

ROC曲线上的每一个点对应于一个threshold，对于一个分类器，每个threshold下会有一个TPR和FPR。

比如Threshold最大时，TP=FP=0，对应于原点；Threshold最小时，TN=FN=0，对应于右上角的点(1,1)

(b)随着阈值theta增加，TP和FP都减小，TPR和FPR也减小，ROC点向左下移动；

　Receiver Operating Characteristic,翻译为"接受者操作特性曲线"，够拗口的。曲线由两个变量1-specificity 和 Sensitivity绘制. 1-specificity=FPR，即假正类率。Sensitivity即是真正类率，TPR(True positive rate),反映了正类覆盖程度。这个组合以1-specificity对sensitivity,即是以代价(costs)对收益(benefits)。 

  此外，ROC曲线还可以用来计算“均值平均精度”（mean average precision），这是当你通过改变阈值来选择最好的结果时所得到的平均精度（PPV）.

　　下表是一个逻辑回归得到的结果。将得到的实数值按大到小划分成10个个数 相同的部分。

| Percentile | 实例数 | 正例数 | 1-特异度(%) | 敏感度(%) |
| ---------- | ------ | ------ | ----------- | --------- |
| 10         | 6180   | 4879   | 2.73        | 34.64     |
| 20         | 6180   | 2804   | 9.80        | 54.55     |
| 30         | 6180   | 2165   | 18.22       | 69.92     |
| 40         | 6180   | 1506   | 28.01       | 80.62     |
| 50         | 6180   | 987    | 38.90       | 87.62     |
| 60         | 6180   | 529    | 50.74       | 91.38     |
| 70         | 6180   | 365    | 62.93       | 93.97     |
| 80         | 6180   | 294    | 75.26       | 96.06     |
| 90         | 6180   | 297    | 87.59       | 98.17     |
| 100        | 6177   | 258    | 100.00      | 100.00    |

其正例数为此部分里实际的正类数。也就是说，将逻辑回归得到的结 果按从大到小排列，倘若以前10%的数值作为阀值，即将前10%的实例都划归为正类，6180个。其中，正确的个数为4879个，占所有正类的 4879/14084*100%=34.64%，即敏感度；另外，有6180-4879=1301个负实例被错划为正类，占所有负类的1301 /47713*100%=2.73%,即1-特异度。以这两组值分别作为x值和y值，在excel中作散点图。

## 决策树基本介绍

1. 决策树是什么？

   ✓以事例为基础的归纳学习算法，着眼于从一组无次序、无规则
   的事例中推导出决策树表现形式的分类规则

2. 决策树能做什么？

   ✓分类器,可以对未知数据进行分类
   ✓预测模型，可以对未知数据进行预测

3. 根据输出变量的类型不同有分类树和回归树：(回归树用的比较少, 而且有很多其他可以替代的算法)

   ✓回归树，输出为连续型变量；
   ✓分类树，输出是分类标签或离散变量。

### 决策树创建过程

![1527732667974](pictures/91)

- 决策树学习采用 自顶向下的递归方式，在决策树的内部结点进行属性值的

  比较并根据不同的属性值判断从该结点向下的分支，在决策树的叶结点得
  到结论。

- 所以从根到叶结点的一条路径就对应着一条分类规则，整个决策树就对应

  着一组分类规则。

优点: 

✓ 分类规则清晰，结果容易理解。
✓ 计算量相对较小，实现速度快。
✓ 非线性，非参数方法。
✓ 不需要预选变量，可处理异常值，缺失值，不同量纲值。

缺点: 

✓ 相比逻辑回归模型，准确性略有不足，常用来做为预
先选变量的方法
✓ 容易过拟合。

可以作为一个辅助的工具, 例如神经网络的挑选变量的过程, 通过搭建一个决策树去实现.

### 决策树的发展

1. CLS	第一次提出使用决策树进行概念学习。
   是许多决策树学习算法的基础。
2. ID3  优化测试属性的选择：熵（信息增益）。
   只能处理离散变量。
   不能处理缺失值。
   偏向于取值较多的属性，容易过拟合
3. C4.5  对ID3进行了改进，优化测试属性的选择：信息增益率。
   可以处理连续变量。
   可以处理缺失值。
4. CART  优先测试属性的选择：基尼系数，最小偏差。
   生成二叉树。
   可以处理连续变量。
   可以处理缺失值。
   可生成分类树，回归树。

### 属性分裂准则

解决优先选取哪种属性划分数据问题。

不纯度函数：i(t) –对系统不确定性、混乱程度的度量

纯度增益：衡量某种特征给系统带来了多少的信息量

![1527733899946](pictures/92)

如何定义不纯度函数？

#### 分裂准则 – 信息增益

信息熵（Information Entropy）,在信息论里面，是对不确定性的
测量。变量的不确定性越大，熵也就越大，把它搞清楚所需要的信
息量也就越大。

信息熵可度量数据划分或训练数据集D的不纯度，取值越小，表明
样本的“纯净度”越高。定义为如下公式：
$$
H(X) = - \sum_{i=1}^{K}P_{x_i}log_2(P_{x_i})
$$
其中𝑃~𝑥𝑗~ 为变量X取值为X~𝑗~ 的值出现的概率。

**适用算法：ID3**

![1527734371634](pictures/93)

信息增益衡量特征给分类系统带来了多少的信息，带来的信息越多，
该特征越重要。
注：某种特征所带来的信息增益越大，说明该特征越能使分类系统
分的开，该特征越重要。

#### 分裂准则 – 信息增益所存在的问题

假设某个属性存在大量的不同值，如ID编号，在划分时将每个值成为一个
结点，如右图所示。

H(P,IDcode)=0
IDcode 的信息增益值=0.88129

这样决策树在选择属性时，将偏向于选择该属性，但这肯定是不正
确（导致过拟合）的。因此有必要使用一种更好的方法，那就是
C4.5中使用的信息增益率。

#### 分裂准则 – 信息增益率

适用算法：C4.5

分裂信息: 

$$
Split(P, X) = - \sum_{i=1}^{M} \frac{|D_i|}{|D|}log_2(\frac{|D_i|}{D})
$$


信息增益率:

$$
GainRatio (P, X) = \frac{Gain(P, X)}{Split(P, X)}
$$

$$
Split(P, IDcode) = -10*(1/10)log(1/10, 2) = 3.3219
$$



信息增益率(IDcode) =0.88129/3.3219=0.2653



#### 分裂准则 – 基尼系数

Gini值越小，表明样本的“纯净度”越高。Gini指标定义为如下公
式：

$$
Gini(D) = 1 - \sum_{i=1}^{m}p_i^2 =  \sum_{i=1}^{m}p_i*(1-p_i)
$$
在只有二元分裂的时候，对于训练数据集D中的属性A将D分成的D1
和D2，则给定划分D的Gini指标如下公式所示：
$$
Gini_A(D) = \frac{|D_i|}{|D|}Gini(D_1) +  \frac{|D_2|}{|D|}Gini(D_2)
$$
上例中如选用基尼系数做不纯度衡量标准，那么不纯度会减少多少？

#### 分裂准则 – 最小方差

在回归树中用最小方差衡量划分的纯度。
以二叉树为例：

![1527782715092](pictures/99)

#### C4.5 算法-总结

Ross Quinlan 在1993提出，它是ID3算法的一个改进算法

- 用信息增益率来选择属性

- 采用后剪枝

- 可生成多叉树

- 优点：相比ID3，其克服了用信息增益选择属性时偏向选择取值多的

  属性的不足，能够完成对连续属性的离散化处理；能够对不完整数
  据进行处理。

- 缺点：需要对数据集进行多次的顺序扫描和排序，因而导致算法的

  低效。此外，C4.5只适合于能够驻留于内存的数据集，当训练集大
  得无法在内存容纳时程序无法运行。

- CART(Classification And Regression Tree) ，Breiman 等人在

1984年提出。

- 用基尼系数（目标变量为离散变量），最小方差（目标变量连续变

量)来选择属性

- 采用后剪枝
- 只生成二叉树，可生成回归树
- 可以处理连续型属性
- 可以处理缺失值

#### 停止条件

为防止树生长过大，导致过拟合，需设置停止条件，称为预剪枝。

1. 如果节点中所有观测属于一类。
2. 如果树的深度达到设定的阈值。
3. 如果该节点所含观测值小于设定的父节点应含观测数的阈值。
4. 如果该节点的子节点所含观测数将小于设定的阈值。
5. 如果没有属性能满足设定的分裂准则的阈值。



## 决策树原理介绍

CART算法是描述给定预测向量后，求取目标变量条件分布的方法。
它使用二叉树将预测空间递归划分为若干子集，随着从根结点到
叶结点的移动，每个结点选出最优的分支规则对应的划分区域，
目标变量在该结点上的条件分布也随之被确定。

![1527738046404](pictures/94)

针对测试属性的不同情况的处理

![1527738086127](pictures/95)

#### 决策树生成算法分成两个步骤：

一是树的生成，开始时所有数据都在根结点，然后递归得对数据进行分支，生成一颗完整的树。

二是选取合适大小的树，就是去掉一些不显著的叶节点或
者子树。

- 设置阀值控制叶节点观测个数

- 交叉验证，利用CCP(Cost-Complexity Pruning)  方法自下而上剪

  枝，优化树的复杂度和分类误差之间的比例，从而调节树的大
  小，避免过拟合。

#### CCP-代价复杂度剪枝法（Cost and complexity)

CCP选择节点表面误差率增益值最小的非叶子节点，删除该非叶子节点的左右子节点，若
有多个非叶子节点的表面误差率增益值相同小，则选择非叶子节点中子节点数最多的非叶
子节点进行剪枝。
➢ CCP方法可以描述如下：
(1) 从原始的决策树开始生成一个子树序列（均非叶子节点）
{T~0~, T~1~, ...., T~n~}, 其中T~i+1~ 从T~i~ 产生，T~n~为根节点
(2) 计算所有节点的误差率增益值，选择误差率增益值最小的非叶子节点。
(3) 对选中的节点进行剪枝。
(4) 重复上边的步骤。
➢ 误差率增益值的计算公式为：$𝛼 =\frac{R(t)-R(T)}{N(T)-1}$
• 𝛼 衡量的是每个节点所能减少的分类误差率
• 𝛼越小，说明该节点所能减少的分类误差率越小，继续往下分类意义不大，可以减
枝，将该结点作为叶结点
• R(t) 表示叶子节点的误差代价，