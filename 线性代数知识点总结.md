# 	4月27日数学基础第一天线性代数和微积分

## 本部分课程学习目标与学习指南

就业方向比较火的: 

1. 商业大数据
2. 人工智能
3. 金融和医药领域(数据量庞大, 需要数据分析的人才)



### 课程共四天，包含内容如下

第一天：预备知识，函数，线性代数

第二天：微积分，数据度量，统计量及抽样分布

第三天：参数估计，假设检验

第四天：列联分析，相关分析，回归分析

### 学习方法

不用学习太多的计算技巧, 主要是掌握原理, 知道怎么回事

最好的自学视频的方法, 是看数学课, 一般教材上面说的都不是人话, 看视频至少是人话, 推荐大家看一些视频课.

中文公开课里面最好的应该是复旦陈纪修教授的公开课, 缺点是课程太长了, 要看起来需要花很多的时间, 不适合快速的学习某些知识.如果想快速, 需要看那些知识点串讲, 网易上面的那种.

### 课程安排

统计分布, 参数估计和假设检验, 这三部分内容是串在一起的, 思路是一脉相承.

第四天的内容, 回归模型, 可能是对咱们以后工作最有用的部分. 

### 学习指南

数学决定了我们工作能力的上限, 编程决定了我们工作的下限, 如果你不会编程, 你可能工作根本找不到.

很多时候你走到头走不下去的原因, 是你的数学能力不够, 可能你需要把你数学模型精进一下, 才能突破天花板.



本部分课程主要为本科阶段高等数学的复习。其中对于数据分析师来说，以下部分是这四天最重要的：**函数 ，线性变换，导数及其应用，矩，抽样分布，最大似然估计，假设检验，线性回归**  。因此请同学们集中精力优先攻克以上内容。

对于数学基础薄弱的同学，学习有以下难点：

1.长时间没有接触和使用数学，尤其大学学过的知识时间太长遗忘太多，导致计算能力偏弱。

2.数学思维仍保持在初等数学范畴，导致无法理解课程中的一些概念与公式。

3.学过的概念与计算很快就忘掉了，导致以后无法应用这两天所学的知识。

 

解决方案：
1.做好预习工作。对于基础不是很好的同学，建议在正式上课之前，从网上找一些高中数学的知识点梳理，看一遍就能快速回忆起之前所学。

2.高等数学由于引入了极限等概念，确实会有一些反直觉的知识存在。建议同学们保持多角度获取知识的好习惯，碰到没听懂的知识点请及时提问，并且课后针对这一知识点尝试去搜索各种角度的解释。很多时候当思考角度改变的时候会有茅塞顿开的感觉。

3.数学是需要练习的，请大家课后一定花时间把老师讲过的推导与计算自己手动重新计算一下。

 

### 提醒：

如果课上或其他时间，你遇到有人提问或讨论学习相关的内容，请务必参与。因为也许你现在会了，但是知识是很会被遗忘的。当你给其他同学解答了疑问，或者相互讨论，那么能抵上你平时十遍的复习。即使你说错了也不用害羞，总比你在工作中犯错要好。

另外，数学课总是相对枯燥的。尤其是前两天的课程，几乎都是数学知识，基本没有相关案例。但是数学是数据分析的基础中的基础，请同学们打起精神，用这四天打牢数学基础，必将获益匪浅。

## 使用教材：

《线性代数应该这样学》，《普林斯顿微积分读本》，《统计学》(贾俊平著)，《统计推断》[^1]

[^1]: 统计推断, 难度比较高, 确实数学符号, 纯数学书, 你看起来会非常的头疼

不要用紫色封皮的那本《线性代数应该这样学》, 这里面一个例题没有, 纯数学教材, 第一章向量, 第二章张量, 第四章张量空间.这个是我们学线性代数的思路, 我们极少会需要大家求个多少维的行列式, 不会有需要同学们做一个多维矩阵的运算, 不用关心运算方面的事情.我们需要向量空间是什么, 我们知道空间映射.

以上教材同学们没有必要全部花钱买实体书，下载一本电子版的书籍留在手边做参考即可。

## 课程内容

古希腊, 万物皆数, 时间所有可量化的东西都能用数字来表示.

第一次数学危机就是无理数的诞生, 第二次是极限.

![1](/pictures\1.jpg)





##集合论： （现代数学从集合论开始推导整个数学体系）

现代数学的基础是集合, 所以现在如果你想看看一个教材是不是一本优秀的教材, 判断的标准就是看看第一章是不是讲集合.

### 数学概况

#### 分析学

>  数学分析(微积分), 数学分析更偏重于证明.微积分更偏重于计算**， 其实这两个是一个东西.
>
>  往后学之后, 百分百要用到的东西叫: 
>
>  实分析（real analysis）: 实分析学习之前必须要用到抽象代数的张量, 所以要先学抽象代数,(可能一个星期就够了)
>
>  如果是工科叫"实变函数", 最少你要在实分析里面学<测度论>（maesure theory）；如果你不会测度论， 现代数学的基础你都不知道。
>
>  现在咱们去看数据分析的前沿论文， 你会发现函数都不写等式了， 全是写映射。很多时候做大数据做不下去了， 很可能是测度论没学会， 一旦你学明白了， 考试及格特别容易， 因为实分析能出的题很少， 如果你有时间， 严重推荐你补一下。
>
>  实分析诞生的初期， 是解决积分问题， 直线上有无数个点， 点的长度是0， 为什么无数个点你能弄出个长度来， 这个就是用实分析解决的。
>
>  复变函数： 复分析（弯曲直线的方法， 直线还是直线， 可以通过弯曲外轴来完成， 它本身是想把数轴弯成一个圆， 然后你不把外轴去那么大， 你把它少弯点， 就可以达成把一条直线变成曲线的方法。）
>
>  复分析对数据分析领域也有一定作用， 例如google做的卷积神经网络， 就是通过复分析做的。
>
>  泛函分析（这个先不学）(在不同空间之间进行变换)：  谁规定我从这个宇宙就还要映射到这个宇宙中， 我就映射到其他的宇宙。
>
>  如果你想做数据科学， 这几部分你可能都得学。
>
>  常微分： 空间中的一个点, 随着时间变化的轨迹, 比如说在股票, K线图, 就是一个波浪.
>
>  偏微分： 描述一个曲面， 一个高维曲面， 随着时间变化的情况。
>
>  主要困难程度就是计算， 难度不太大，
>
>  流形: goggle 新弄的东西, 都是用流形在做的.
>
>    

#### 代数学

> 线性代数(任何一本书都可以告诉你, 计算是如何算的, 我们主要掌握空间是如何变换的, 线性代数我们就不提计算.)
>
> 继续往后学-->
>
> 抽象代数: 主要两章内容, 张量代数, 群论.张量代数式线性代数的发展, 矩阵再往下发展, 叫高阶张量.群论是研究运算规则的, 一只猫加一只狗等于两只猪, 是研究运算规则的.群论看完前三章就可以了.看完阿贝尔群子群直接不用看了.

几何： 基本上没什么用， 高等几何基础是拓扑学， 是研究几何不变量的. 

## 数据类型第一种划分方法： 

### 离散型数据: N自然数

### 稠密性数据:  Q 有理数, 中间有无理数

### 连续性数据:  R 实数, 实数轴没有空隙

绝大部分时候, 我们会把后两种都叫做连续性数据.

## 数据类型划分方式2：

###1.  定类数据 ： 

特点， 无序， 无法计算， 例如国籍

###2. 定序尺度 ：
特点， 有序， 但是无法计算， 例如健康状况
###3. 数值尺度 ： 
特点， 有序， 并且可以计算， 例如身高

##数学模型的分类(商业领域中常用模型) 

###1.横截面数据：

特点， 一个时间点或一个时间段内收集的不同地方的数据， 例如按省份收集的数据。

我们在数据分析中绝大多数用到的都是横截面数据， 例如alphago 把每一个格子都算出一个获胜的概率出来， 本质上就是在预测一个模型， 根本就不关心时间序列的问题。

> 在算法中的应用：  **多元回归, 逻辑回归, 托宾回归(大多数人都选0, 只有少数人选1), 截尾回归(我发调查问卷的时候, 就把这个尾巴给截断了, 用截尾回归)**
>
> 多元回归， 逻辑回归， 托宾回归（大多数人都选0， 只有少数人选1的情况下应用）， 截尾回归（发调查问卷时发现关键数据没有进行调查， 例如最大划分100万收入， 但我们需要300万收入以上的数据）
>
> 托宾回归: 解决云膨胀的问题.淘宝首页第一个滑动窗口的广告, 点击率1%左右, 解决一个广告的投放的效果, 最重要的就是点击率吧, 如果你项评价一个点击率, 你会碰到一个云膨胀的问题, 大多数人点0次, 你绝大部分人都是点击数为0, 你也能跑个回归模型出来, 但是效果很不好, 托宾回归专门解决这种云膨胀的问题.
>
> 截尾回归: 您的年收入是__, 几乎没有人会在这个地方填数字, 应该方法, 分类, 您的年收入属于以下哪一档.大部分人其实还真会选一个自己年收入附近的档.所以以后你采集敏感数据, 你可以这样去处理, 你企业定义贵宾客户是, 年收入300万以上, 但是你的数据最大的那档是100万以上.你没有300万以上的数据, 那么你怎么处理?那个尾巴被我给截了, 截尾回归, 就是专门用来解决这个问题.
>
> 

### 2.时间序列数据： 

特点， 按照时间先后进行划分.例如股票的市值, 随着时间变动, 比如说去年一年的销量, 

> **ARIMA(插分, 回归值)
>
> ARIMA就是我们最基础的时间序列回归.
>
> 协整模型: ARIAM一次只能分析一个时间序列, 如果你想把多个时间序列放到一起来讨论的话, 你可以用协整模型
>
> GARCH: (分析商业领域里面的羊群效应, 突然某个板块被炒热了, 投资者会蜂拥而至, 波动性会急剧增大, 随着热度消逝, 它的波动性又会变回去, 专门用来分析波动性的), 非常常见的时间模型, 很常用了, 习惯性的做一个ARIMA, 在做一个GARCH, 进行比较.

### 3.面板数据类型(商业中基本不太用): 

既有时间属性有用空间维度的数据, 你把它拆开好多个时间来做， 你会割裂不同地点之间的关系。

很多问题科学家都没太搞明白。 

 固定效应/随机效应/, 空间计量模型, 因为模型不太成熟, 应用 中有可能会出现错误, 因此几乎不会应用。

### 4.结构方程模型: 

当我有多个Y值的时候, 解的不是一行模型了, 解的是一个方程组**

### 5. 向量自回归模型(VAR): 

当Y值变化时, 会对X产生影响, 所以其实并不区分X和Y



## 1.传统统计模型

> 所有的回归模型, 我们都把它归为传统统计模型, 
>
> 解决过去, 分析过去, 例如什么因素, 导致我企业销量降低, 我们就想把这些因素里面, 显著影响我销量的因素分析出来.由过去的经验, 总结出一个什么东西.
>
> 传统统计模型, 两类问题都能解决.
>
> 95%的数据挖掘模型, 只能预测未来, 不能分析过去.
>
> 但如果只是用来预测未来, 数据挖掘可以解决更多的问题, 可以给出更精确的预测.
>
> 而我们一般情况下都希望能预测未来.



## 2.数据挖掘模型

> 解决预测问题: 决策树, 神经网络, 支持向量机, 贝叶斯网络, 关联规则
>
> 预测, 这个用户有多大概率购买我的产品, 我走这步棋, 则我获胜的概率是多少.



数据分析其实做到最后， 就是在不同空间之间的转换， 我们把一个数据库做成一个数据空间， 然后我们可以有各种各样的方法进行转换， 

## 回归模型

回归模型有很多, 最简单的回归模型是一元线性回归模型: 

举些例子来理解模型：

>  LOL胜率=β_0+β_1 练习"时间"+ϵ
>
> 练习时间每多一分钟，会使胜率提高β_1个百分点。

> 皮肤光泽度=β_0+β_1 燕窝摄入量+ϵ
>
> 每多吃一斤燕窝，会使皮肤光泽提高β_1度。

![2](/pictures\2)





##数据挖掘模型

**支持向量机, (二分类模型) , 核函数解决低维到高维投影问题**

#线性代数

## 1.向量

对于向量有三种定义: 

> 物理: 有方向的量
>
> 统计学: 空间中的点
>
> 计算机: 数表

只要我们默认向量的起点在原点, 在数学中我们就可以将以上三种向量的概念结合到一起.

### 向量运算

####向量的加法: 

在二维空间中, 向量的加法就是求给定两个向量所围成的平行四边形的对角线, 直接把对应数值进行相加.

#### 向量的数乘: 

将给定向量按比例缩放（拉伸），负数表示反向拉伸。

所有给定向量的集合，叫向量空间。

## 2.线性组合

###首先线性的定义: 

1. 对加法和数乘封闭, 就是说你不管怎么加乘, 不会跑出这个空间之外去.


2. 因此向量空间是线性的

单位向量: 数轴上的单位长度, 例如(1, 0)

零向量: 原点, 即向量的所有元素都为0

我们可以简单的认为, 每个单位向量都是坐标系的一个基向量. 所有基向量的组合叫基组, 基组定义了该向量空间.

在线性空间中，所有其他向量都是以上两种向量的线性组合。

或者说，用单位向量和零向量，可以通过加法和数乘组合出任意一个其他向量。

###张量与长成空间

以二维空间为例：在线性组合中，给定两个非零向量。

一个向量固定，另一个向量自由变化，其线性组合可得到一条直线。但是，令两个向量都自由变化，可以得到一个平面吗？

大多数情况下确实如此，除非两向量共线。

####张成空间：

所有可以表示为给定向量线性组合的向量集合，被称为给定向量张成(span)的空间。

若给定多个向量，移除其中一部分而不减小张成空间，是为线性相关。

如果所有向量都给张成空间增加了维度，是为线性无关。

####基组的另外一个定义：

向量空间的一个基组是张成该空间的一个线性无关的向量集。

## 3.线性变换与矩阵

变换就是跃迁, 线性变换后, 直线依旧是直线, 且原点不变.

变换一个向量有两种方式: 

 	1. 将该向量旋转拉伸
	2. 改变整个坐标系, 这是操纵空间的手段

在线性空间中, 改变空间只需要改变基组, 也就是改变基向量的方向和长度.

变换的是基组。因此变换后，可通过新的基组求出新的向量。

换句话说，只要知道基组的新落脚点，我们就可以推断出任意向量的新落脚点。

因此，矩阵代表一次变换。用一个向量左乘一个矩阵，就表示将这个向量按此矩阵所定义的变换映射到新的向量。

## 4. 矩阵乘法

矩阵乘法就是	复合变换: 两个变换先后作用

两个基组同时转动就是旋转矩阵, 一个基组旋转, 一个基组不变, 就是剪切

## 5.行列式

既然矩阵可以将空间做伸缩, 我们不禁要问: 

一次线性变换中, 究竟空间被拉伸/压缩了多少?

以2维空间为例, 既然线性变换是改变基组, 那么我们只需要找到一个指标来度量, 2个基向量围成的矩形面积, 增大或者缩小的比例, 就是整个空间把拉伸或者压缩的比例.

该指标其实就是行列式的值: 

记作 det().

下面这两个是公式的案例, 和本处内容无关.

$$ \Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,. $$

$$
\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,
$$


例：矩阵 

$$ \left( \begin{array}{ccc} 3 & 0  \\0 & 2 \\\end{array} \right) .$$ 就是将基组围成的矩形面积扩大了6倍.因此该矩阵的行列式为6

而矩阵: $$ \left( \begin{array}{ccc} 1 & 1 \\0 & 1 \\\end{array} \right) .$$ 只是让空间倾斜，但矩形面积不变，仍为1.

若行列式的值为负，表示空间发生了翻转，交换了坐标轴的顺/逆时针顺序。

若行列式的值为0，则是将平面压缩维数变成直线，矩形面积当然就为0.
因此检查矩阵行列式的值是否等于零，可以检验该矩阵的变换是否将空间降维。


det(MN)=det(M)det(N)
两个矩阵 M, N，积的行列式=行列式的积

行列式行数必须等于列数。也就是说必须是方阵才有行列式。

## 6.逆矩阵和矩阵的秩

如果我们课程找到一个A逆, 另等式两边同时乘以A逆, 这样就求出X的值了.

并不是所有的矩阵都能找到逆矩阵

线性方程组可以转化成矩阵形式  𝐴𝑥 ⃗=𝑣 ⃗ .
也就是找到一个向量𝑥 ⃗，使它在经过矩阵A变换后，成为𝑣 ⃗ 。

实际计算中，我们其实找的是逆变换𝐴^(−1)，使𝑣 ⃗ 经𝐴^(−1)变换后得到𝑥 ⃗。
𝐴^(−1)叫做A的逆矩阵。

𝐴^(−1) 𝐴 必须等于什么都没做。

那么，在所有矩阵中主对角线元素为1，其他元素都为0的矩阵，就代表什么都不做的变换。
这种矩阵称为单位矩阵，记作I或者E.

所以 𝐴^(−1) 𝐴=𝐼

但是，在线性代数里，当det(A)=0时，空间被降维后就没有对应的逆变换𝐴^(−1)能变回去了。

如果我空间被降了一个维度, 假设逆矩阵存在, 我就是要把一维空间变回去, 现在这条直线上每一个点, 都承载着原来的那个线上的全部信息, 你想直接展, 确实展不出来了, 等于说信息损失了.

我们找不到一个空间A逆, 使它变回去, 所以我们说如果行列式为零, 我们就称为不可逆.

因此当det(A)=0，逆不存在。

不可逆矩阵称为奇异矩阵。
可逆矩阵称为非奇异矩阵。

#### 矩阵的秩 ：线性变换后空间的维数。

满秩：秩与矩阵的列数相等

例如，对于2*2矩阵，秩最大为2.也就是基向量最多张成整个2维空间，此时det≠0.
对于一个满秩变换来说，唯一能在变换后落在原点的是零向量自身。(矩阵的核就是零点本身, 没有任何其他东西被压缩到原点里了, 也就是说我们现在知道, 这是第三个办法可以判断空间是否被降维了.(1.线性相关 2.行列式是否为0 3.看是否满秩))

不满秩：变换后维度降低，此时det=0.
若不满秩，则有很多向量被压到原点。

核（Kernel):

降维后, 原点中承载了原来一条线上的全部信息, 这个核函数就是这个核.

核函数就是想一些办法把被压缩的那些点给摘出来, 这就是核函数干的事情.

在变换后落在原点的向量的集合，称为该矩阵的“核”，也叫零空间.

## 7. 非方阵

如果是非方阵代表了什么呢?非方阵代表了不同维度空间之间的线性变换.

一个3*2矩阵, 它代表二维映射到三维

2*3的矩阵, 表示三维映射到二维

这个映射和那个和刚才那个降维的映射不是一件事情, 这种映射是要求这两个空间已存在, 你才能给出的映射, 可以来回映射, 不会有信息损失.

投影矩阵就是说这个, 把一个三维空间的东西投影到二维平面.

当矩阵是1*2时, 是2维到1维的变换.

例:
$$
\left( \begin{array}{ccc}
a & b  \\
\end{array} \right)

\left( \begin{array}{ccc}
c   \\
d \\
\end{array} \right)

= ac + bd
$$
等于把1个二维向量压缩到1维中, 其实就是点乘.

## 8. 点乘内积

点乘也叫内积。

2个向量点乘，例如 𝑣 ⃗∗𝑤 ⃗ 就是把向量𝑤 ⃗向向量𝑣 ⃗作投影，此时

内积=投影长度 * 𝑣 ⃗的长度

点乘满足交换律。

点乘也可以看作把空间压缩到一维。

**点乘, 就是以一个向量向另外一个向量做投影, 用一个投影的长度, 乘以被投影的向量长度, 两个长度相乘就是点乘(内积).如果算出来是负的, 说明是异侧投影.**

### 张量

其实就是矩阵维度

标量 a --> 在张量中, 叫零阶张量

向量   --> 在张量中, 叫一阶张量
$$
\left( \begin{array}{ccc}
a  \\
b \\
b 
\end{array} \right)
$$


矩阵  --> 在张量中, 叫二阶张量
$$
\left( \begin{array}{ccc}
a & d  &g\\
b & e &h \\
c & f &i \\
\end{array} \right)
$$
三阶张量是由二阶张量拼出来的

**张量是用来研究, 在线性变换中, 不变的那些量.**

举个例子, 平行, 你无论怎么做线性变换, 两条线都得平行.(这里的平行不是除非的那种平行)



## 9. 叉乘, 外积

叉乘用当很少, 两个向量叉乘的结果得到一个P向量, P垂直于原始两个向量定义的那个平面, P向量的长度的数值, 等于VW向量围成的平行四边形的面积的数值.如果我们能够找到这样的一个P向量, 则P向量就叫做V W的叉乘, 也叫外积.

## 10.特征值与特征向量

线性变换中，大部分向量在变换后会离开它的张成空间，但也有一些会留在它所张成的空间中。

但是还有一些它的方向没有发生变换, 这些没有发生变换的向量, 就叫做**特征向量**.

这些向量没有出它的张成空间.

经历一次线性变换以后, 没有离开它所在的张成空间的那些向量, 就叫做特征向量.

特征值: 所谓**特征值**就是在线性变换过程中, 这些特征向量, 变化的比例, 长度被拉伸或者压缩了多少倍.

例如
$$
\left( \begin{array}{ccc}
3 & 1  \\
0 & 2 \\
\end{array} \right)
$$
绝大部分向量在变换后都离开了该向量的张成空间，例如Y轴的单位向量。
但也有2个向量留在了它们分别的张成空间中，这两个向量是
45度线
$$
\left( \begin{array}{ccc}
1   \\
-1  \\
\end{array} \right)
$$
 X轴
$$
\left( \begin{array}{ccc}
1  \\
0 \\
\end{array} \right)
$$
上例中， 第一个向量被拉伸了2倍，则叫做属于特征值2的特征向量。

 第二个被拉伸了3倍，则(1, 0)就是属于特征值3的特征向量。

公式  𝐴𝑣 ⃗=𝜆𝑣 ⃗

特征向量左乘这个矩阵, 和这个向量只是拉伸长度, 是相等的, 那么我们找到的就是特征向量.

注意：

1. 并不是所有的矩阵都有特征值和特征向量。
2. 如果所有的基向量都是特征向量，此矩阵一定是对角阵，例如空间逆时针旋转90°
3. 更高维度的空间中，**特征向量往往是变换的旋转轴。**

空间绕着这个向量旋转, 所以是旋转轴, 所以它是一个线性变换的核心啊!

## 需要掌握的计算

1. 向量运算

   - 向量的加法数乘, 对应相加, 乘进去就可以

2. 矩阵的加法, 数乘, 矩阵乘法

   - 矩阵的加法, 第一个你得一边大, 然后对应元素相加.数乘也一样.

3. 矩阵的转置

   - 交换行和列的位置

4. 矩阵的初等变换(高斯消元)

   - 把方程式系数提出来组成一个矩阵, 然后把右边的加进去组成一个增广矩阵.

   - 矩阵的初等变换你只能做三件事情**(初等行变换)**

     1. 交换某两行
     2. 数乘某行
     3. 某两行相加相减

     如果你可以把矩阵的左下三角变成0, 那这个矩阵就出来了.

5. 求矩阵的逆

    - 也可以通过初等行变换来表示, 如果你考研有各种各样的技巧, 最初等的方法就是通过初等行变换来求矩阵的逆.

    $$
     \left( \begin{array}{ccc}
     1 & 3  \\
     2 & 7 \\
     \end{array} \right)
    $$


















也要先增广矩阵, 右边增加一个单位矩阵I
     $$
     \left( \begin{array}{ccc}
     1 & 3 &1 & 0  \\
     2 & 7 & 0 & 1\\
     \end{array} \right)
     $$
如果我通过初等行变换把左边变成对角矩阵, 右边就是他的逆.
     $$
     \left( \begin{array}{ccc}
     1 & 0 &7 & 3  \\
     0 & 1 & -2 & 1\\
     \end{array} \right)
$$

$$
     \left( \begin{array}{ccc}
     7 & -3  \\
     -2 & 1 \\
     \end{array} \right)
     \left( \begin{array}{ccc}
     1 & 3  \\
     2 & 7 \\
     \end{array} \right)
     =
     \left( \begin{array}{ccc}
     1 & 0  \\
     0 & 1 \\
     \end{array} \right)
$$

​     

6. 求矩阵的秩(行阶梯型与行最简形)

   1. 行列式能不能削成0

7. 求矩阵的特征值和特征向量(上面有公式) 公式  𝐴𝑣 ⃗=𝜆𝑣 ⃗

   - 先把λ变成一个矩阵, 变成一个单位矩阵

     ![1525515225946](/pictures/11)

     - ![1525515318234](C:/Users/ADMINI~1/AppData/Local/Temp/1525515318234.png)

   - 把λ回代回去, 就可以把v向量解出来.



## 补充: 奇异值分解

这个地方告诉我们奇异值分解是怎么回事, 我们这个地方告诉大家, 奇异值分解到底是什么东西.

奇异值分解实际上把矩阵的变换分为了三部分：

- 旋转
- 拉伸
- 投影（方阵没有投影）(如果是不同维度之间的变换)

把一个矩阵拆成了三个矩阵, 做乘法.

矩阵乘法就是从右往左, 一个一个做线性变换.

中间这个矩阵一定是一个对角矩阵

或者说
奇异值分解，就是把矩阵分成多个“分力”
奇异值的大小，就是各个“分力”的大小

![3](/pictures/3)

![1525513618178](/pictures/10)



两边的矩阵都是对空间进行一次旋转, 中间的是对空间进行拉伸.

最右边这个矩阵是把空间逆时针旋转了90°, 中间这个是拉伸矩阵, 把空间拉伸的多少倍.最左边, 把这个空间再倾斜这么大一个角度, 

我们这样分解有什么作用呢?

如果我们在变换之前, 那么笛卡尔坐标轴中, 它的X和Y是同等重要的, 各承担了, 这个数据50%的信息, 但我们经过这次变换之后, 这个X和Y轴就不是同等重要了, 变换一个X轴承载2/3的信息, Y轴承载了1/3的信息, 因为X轴更宽了, Y轴更窄了.

如果我必须把X轴和Y轴的某一个给去掉, 如果你没有经过这次变换, 你不管去掉哪个, 你都损失了50%的信息, 你经过了这次变换以后, 就只损失了30%的信息, 可以更有效的保留我们的信息了.

做主成分分析(PCA)也是这个思路, 不过主成分分析师用特征值特征向量来找的, 其实效果是一样的.

图像压缩就是这样的思路, 奇异值分解后删除后信息损失的很少.





