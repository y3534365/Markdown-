# 课堂笔记(已经回忆不起来内容了)



# 3-5日课堂笔记

计算机行业，可能需要终身学习

 

数据库模型：

最早的

网状模型

层次模型

```
关系模型：Oracle，db2（小型机）,sql Server,mysql, inno db,
NOsql:hive hbase pig
 
Linux : redhat （红帽子）商用，基础版免费
Centos 红帽的变种 早期的改造一下作为免费版
Ubuntu 个人版 免费
 
Sudu apt-get update   第一步 更新系统
Sudu apt-get install mysql-server  安装mysql服务
Sudu mysql_secure_installation ﻿
```

 

 

我们不学的但很重要的   数据安全 审计 用户管理  这些我们不涉及到。

 

 

Excel无法满足多用户 和同时修改

 

mysql数据库端口号3306

oracle:1521

sql server : 1433    这是一个常识性问题

 

在线安装，简单，但实际用不到

离线安装，工作方式，难度很高，复杂。

 

数据库中的表类型

 

堆型表：我们默认的表 都是堆型表

 

索引组织表 ：在某一列上加了一行索引  当插入一个新数据后，后面位置的数据将会被依次向后移动位置

主键表：在主键一行，不允许重复

 

分区表：用多个小表组成的一个大表，或者用一个大表分割成的多个小表。（主要目的是提高查询效率）

 

增加外键表示一对多关系

如果需要可以定义约束条件，如果想要高效使用数据库不建议加。

固定业务，长期业务，不变更的业务，才建立主外键关系。

 

 

## 评价数据表设计的质量

1. 数据库中冗余的数据需要额外的维护，因此质量好的一套表应该尽量“减少冗余数据”。
2. 经常发生变化的数据需要额外的维护，因此质量好的一套表应该尽量 “避免数据经常发生变化”

 

## 使用规范化减少数据冗余

函数依赖：一张表内两个字段值之间的一一对应关系称为函数依赖。

第一范式：一张表内同类字段不重复出现，

第二范式：每个“非关键字”字段“仅仅”函数依赖于主键。

第三范式：如果一张表满足第二范式的要求，并且不存在“非关键字”字段函数依

赖于任何其他“非关键字”字段，那么该表满足第三范式的要求。

 

 

Mysql服务---数据库—表

每一个数据库都有自己的字符集

 

Show character set;  查看字符集

show variables like 'character%';  看当前使用的字符集

set character_set_client = gbk;  修改

 

## SQL 基本的执行方法（两种）：

\. C:\mysql\init.sql

 source C:\mysql\init.sql

 

删除全部数据  delete from t1;  生成大量日志，删除慢 数据可恢复

截断表 ：  truncate table t1;  一条日志，快，不可恢复

删除表 ：Drop table t1;   删除数据和表结构，快，不可恢复

 

 

### 离线安装mysql过程

\1. 配置 mysql数据库的拥有者  mysql用户

sudo useradd mysql

1. 移动到特定目录

sudo mv 文件名 /usr/local

打开文件夹：cd /usr/local  查询文件列表： ll

1. 解压缩： sudo tar –zvxf 文件名

重命名 ： sudo mv 文件名 新文件名

1. 更改主属权限： sudo chown –R mysql mysql
2. 更新缺失文件： sudo apt-get install libaio1
3. 安装mysql文件：sudo ./scripts/mysql_install_db -- user=mysql

\7. 配置 linux操作系统可以启动和关闭mysql数据库

sudo cp support-files/mysql.server /etc/init.d/mysql

\8. 尝试启动mysql 数据库

sudo service mysql start

1. 登录mysql ./bin/mysql –uroot –p123

# 3-6日课堂笔记

数据类型：

int 整数型 其他的很少用

float (5,2) 小数的整数位加上小数的小数位，一共长度是5，其中小数位长度是2

char 定长长度字符串 ，没填满的部分填满空格。

varchar 变长长度字符串

date 表示日期，默认格式为‘YYYY-MM-DD’； ；

time 表示时间，格式为‘HH:ii:ss’； ；

year 表示年份

datetime 与timestamp 是日期和时间的混合类型，格式为'YYYY-MM-DD HH:ii:ss'

 

复制表结构：create table t2 like t1;

复制后表是空的

复制一张表带数据：create table t3 select * from t1;

 

如果为表增加列 会急剧降低性能(行迁移)

alter table 表名 drop 字段名 (在生产薄上不要做)

alter table  表名 add  新字段名 型 新数据类型 [  新约束条件 ] [ first | after  旧字段名]

alter table  表名 change  旧字段名  新字段名  新数据类型

alter table  表名 modify  字段名

alter table  表名 add constraint  约束名 型 约束类型 (字段名)

（1 ）删除表的主键约束条件语法格式比较简单，语法格式如下。

alter table  表名 drop primary key

（2 ）删除表的外键约束时，需指定外键约束名称，语法格式如下（注意需指定

外键约束名）。

alter table  表名 drop foreign key  约束名

修改表名：

rename table 旧表名to 新表名

该命令等效于：alter table  旧表名 rename  新表名

 

 

 

## 索引

创建索引

B树索引：(可以建在数值型和字符型的字段上)不在日期字段上建索引

数值和字符本身要是散列分布的均衡数据，例如序列性数值，重复非常少的随机数值，

create index kk on t1 (id)

位图索引 bitmap：分布不均衡的数据，例如性别，学历，质量，

create bitmap index kk on t1 (id)

函数索引：select * from t1 where mod (id) = 0

Create index kk on t1（mod（id,2））

mod 取余数

create index kk on t1 (id)

 

索引的设计往往需要一定的技巧，掌握了这些技巧，可以确保索引能够大幅地

提升数据检索效率，弥补索引在数据更新方面带来的缺陷。

原则1 ：表的某个字段值离散度越高，该字段越适合选作索引的关键字。

原则2：占用储存空间少的字段更适合选作索引的关键字。

原则3 ：较频繁地作为where 查询条件的字段应该创建索引，分组字段或者排序

字段应该创建索引，两个表的连接字段应该创建索引。

原则4 ：更新频繁的字段不适合创建索引，不会出现在where 子句中的字段不应

该创建索引。

原则5 ．最左前缀原则

原则6 ．尽量使用前缀索引

 

 

数据库sql语言：数据定义语言，数据操纵语言 数据控制语言

 

数据定义语言：create database；create table;create index,create user,

 

数据控制语言：grant revoke 权限处理；commit rollback；

 

数据操作语言：增insert删drop改 (增和删结合起来)查 (重点)

 

insert into  表名 [ （字段列表）] values

insert into  目标表名 [( 字段列表 1)]

select ( 字段列表 2) from 表 源表  where  条件表达式

 

**replace 语句的语法格式有三种语法格式。**

 

语法格式1 1 ： replace into 名 表名 [ [ （字段列表） ] values  （值列表）

 

语法格式2 2 ： replace [into]  目标表名 [( 字段列表 1)]

select ( 字段列表 2) from 表 源表  where  条件表达式

 

语法格式3 3 ：

replace [into]  表名

set  字段 1= 值 1,  字段 2= 值2 2

 

**replace 语句的功能与 insert 语句的功能基本相同，不同之处在于：使用**

**replace 语句向表插入新记录时，如果新纪录的主键值或者唯一性约束的字段值**

**与已有记录相同，则已有记录先被删除（注意：已有记录删除时也不能违背外键**

**约束条件），然后再插入新记录。**

 

**4.2  表记录的修改**

 

**1、基本格式**

**SELECT [ ALL | DISTICT ] <字段表达式1>**

**[，<字段表达式2> [，…] ]**

**FROM <表名1> [，<表名1> [，...] ]**

**[ WHERE <筛选条件表达式> ]**

**[ GROUP BY <分组表达式> [ HAVING <分组条件表达式> ] ]**

**[ORDER BY <字段> [ ASC | DESC ] ]**

 

**2、语句说明**

- **SELECT语句的基本格式是由SELECT子句、FROM子句和**

**WHERE子句组成的查询块。**

 

**整个SELECT语句的含义是：根据WHERE子句的筛**

**选条件表达式，从FROM子句指定的表中找出满足条件记**

**录，再按SELECT语句中指定的字段次序，筛选出记录中**

**的字段值构造一个显示结果表。**

**● 如果有GROUP子句，则将结果按<分组表达式>的值进**

**行分组，该值相等的记录为一个组。**

**● 如果GROUP子句带HAVING短语，则只有满足指定条**

**件的组才会显示输出。**

**提示：SELECT语句操作的是记录（数据）集合（一个表或多个表），**

**而不是单独的一条记录。语句返回的也是记录集合（满足Where**

**条件的），即结果表。**

update  表名

set  字段名1= 值1, 字段名2=值 值2,….. , 字段名n= 值n

[where  条件表达式]

where 子句指定了表中的哪些记录需要修改。若省略了where 子句，则表示修改表中的

所有记录。

set 子句指定了要修改的字段以及该字段修改后的值。

 

 

 

查询：select 列名,列名 from 表名; (分号代表sql语句的结束)

考虑代码的强健性  * 符号要注意

 

通常我们会限制输出：限制列(在select关键词后面添加输出列名) 限制行(在后面添加where条件子句)

限制数值 > < =

限制字符 name= 'a' 单引号是字符型数据必须的

日期型数据 

 

如果想具有通用性，日期需要函数处理

 

使用别名：

 

select name "名字"

 

区间值 between  注意是闭区间

 

集合值 in (逗号分隔)

 

模糊查询  like 'b%f';

% 匹配任意0到多个数值   _ 匹配任意多个字符

 

空值与任何值的计算结果都是空 null+1 等于  null

 

逻辑运算

与、或、非

and、or、not

 

### 一些常用的函数：

连接两个字段：concat(字段名1，字段名2)

upper()  把小写变成大写

lower()  变成小写

where 后面加binary可以大小写敏感

select id,name,sale_date from t1 where binary name = 'A';

 

length()

sum、max min avg variance方差 std标准差 id%2 取余 mod(id,2)取余

 

date_add(birthday,interval -2 day) 两天之前 +2 day +2 month

 

ifnull(name,'HHHHH')  把空值改为函数值

 

 

 

## 分组查询

数据分布不均衡

select name,count (id) from t1 group by name order by 2 | count (id);

 

只要条件子句 包含聚合函数，引导的关键词 用having

 

所有select之后出现的列名中 没有用聚合函数处理的都必须出现group by

 

having需要出现 order by 之前

 

 

## 多表查询

 

关键词用 on t1.id = t2.id

内连接：内连接的特征是，参与输出的表，每个表输出相同的数据行；inner join

 

外连接：左外连接，右外连接，全连接

 

区别，两个表里面的关键词。

 

 

## 子查询

有两个以上的查询语句，通常出现在条件子句中

 

子查询 一个值 用 =  多个值 用 in  比较关系 匹配全部返回真 all  匹配任意一个返回真 用any

 

## 集合运算

```
1
2
3









mysql>select*fromtt1unionselect*fromtt2;



mysql>select*fromtt1unionallselect*fromtt2;
```

 

 

## 视图：

视图是一个定义，没有实体，常用于限制权限的扩散。

物化视图（优化查询）

mysql> select tt1.id id11,tt1.name name11,tt2.id,tt2.name

from tt1 inner join tt

on tt1.id=tt2.id;

 

mysql> create view qq_v as select tt1.id id11,tt1.name

name11,tt2.id,tt2.name from tt1 inner join tt

on tt1.id=tt2.id;

 

mysql> select * from qq_v;

 

mysql> drop view qq_v;

 

mysql> select * from qq_v;

## 存储过程

在查询之前，先定义一个变量，

把一个sql语句固化下来，以后重复执行的时候，执行速度会变得非常快

mysql> delimiter //

mysql> create procedure aa(in in1 int,out out1 int )

​    -> begin

​    -> select count(*) into out1 from t1 where id=in1;

​    -> end;

​    -> //

Query OK, 0 rows affected (0.00 sec)

 

mysql> call aa(1,@out1);

​    -> select @out1;

​    -> end;

​    -> //

Query OK, 1 row affected (0.01 sec)

 

+-------+

| @out1 |

+-------+

|     1 |

+-------+

1 row in set (0.01 sec)

 

 

插入多行数据

原则上不要一次插入多行数据，要么同时成功，要么同时失败

 

合并数据：

insert into t2 select * from t1;

 

replace插入新纪录：歧路

 

sql语言 语法并不难 但是 难在业务表达上。

 

## 删除数据

表记录的删除，delete语句删除的数据可以使用方法恢复

Truncate 删除不能恢复

 

 

聚合函数：多个输入参数，一个输出结果。

 

 

 

人的短期记忆：第一次36小时，一周，一个月，三个月，

 

# 3-10日课堂笔记

## Java语言介绍

Java语言是一种刚需，大数据框架，基础框架都是由java开发的。

Spark框架是基于scala开发的，但是scala是基于java开发的。

 

Storm处理数据很快。

语言：人与人之间的用的沟通方式

机器语言：人与计算机交流的方式

计算机不懂人的语言，所以需要一个翻译。

Ruby perl

Java 应用非常广泛，是目前主流性的语言。

学了java可以做任何事情，几乎。

Java是由SUN(Stanford University Network),斯坦福大学网络公司推出的高级编程语言。

面向internet的编程语言。

我们只需要知道，Hadoop需要java的哪部分知识，进行了解就可以了。

Java的范围非常大，因为它的框架很广泛。

Python 的网络平台框架

跨平台性：Java是一个与平台无关的平台语言，如果是微软开发的就比较恶心。

C#是微软开发的，所以不通用

跨平台原理：先安装一个JVM java virtual machine即可。

由JVM来负责运行

 

JRE(java runtime environment)包括JVM的java程序所需的核心类库等

JDK(java development Kit) 是提供给

 

1. 将java代码编写到.java的文件中
2. 通过javac命令对该java文件进行编译。
3. 通过java命令对生成的class文件进行运行
4. 

 

Java对大小写敏感.

class 类

 

## java代码编写

### 关键字:

定义:被java语言赋予了特殊含义的单词

特点:关键字中所有字母都为小写

Class interface byte short int long float double char boolean void

 

数据类型的关键字:true false null

 

流程控制:

If else switch case default while do for break continue return

 

定义访问权限:

Private protected public

定义类,函数变量:

Abstract final static synchronized

 

类与类之间的关系

Extends implements

 

定义建立实例及引用实例

New this super instanceof

 

异常处理:

Try catch finally throw throws

 

用于包的关键字:

Package import

 

其他修饰符关键字:

Native strictfp transient volatile

 

### 标识符

l  在程序中自定义的一些名称。

l  由26个英文字母大小写，数字：0-9 符号：_ $ 组成

l  定义合法标识符规则：

1，数字不可以开头。

2，不可以使用关键字。

l  Java中严格区分大小写。

l  注意：在起名字的时，为了提高阅读性，要尽量有意义。

Java中的名称规范：

l  包名：多单词组成时所有字母都小写。

- xxxyyyzzz

l  类名接口名：多单词组成时，所有单词的首字母大写。

- XxxYyyZzz

l  变量名和函数名：多单词组成时，第一个单词首字母小写，第二个单词开始每个单词首字母大写。

- xxxYyyZzz

l  常量名：所有字母都大写。多单词时每个单词用下划线连接。

- XXX_YYY_ZZZ

 

### 注释

l  用于注解说明解释程序的文字就是注释。

l  提高了代码的阅读性。

l  Java中的注释格式：

- 单行注释
- 格式： //注释文字
- 多行注释
- 格式： /*  注释文字  */
- 文档注释
- 格式：/** 注释文字 */

单行注释 //

多行注释  /*  注释文字 */

文档注释  /** 注释文字 */

l  注释是一个程序员必须要具有的良好编程习惯。

l  初学者编写程序可以养成习惯：先写注释再写代码。

l  将自己的思想通过注释先整理出来，在用代码去体现。

l  因为代码仅仅是思想的一种体现形式而已。

 

### 常量与变量

Java语言是强类型语言，对于每一种数据都定义了明确的具体数据类型，在内存总分配了不同大小的内存空间

l  自动类型提升

**byte b = 3;**

**int x = 4;**

**x = x + b;**//b会自动提升为int类型进行运算。

l  强制类型转换

**byte b = 3;**

**b = b + 4;**//报错

**b = (byte)b+4;**//强制**类型**转换，强制将b+4的结果转换为byte类型，再赋值给b。

l  思考：

**byte b1=3,b2=4,b;**

**b=b1+b2;**

**b=3+4;**

**哪句是编译失败的呢？为什么呢？**

 

### 算数运算符

​                                                    

### 赋值运算符

l  符号：

​              = , +=, -=, *=, /=, %=

l  示例：

​              int a,b,c;  a=b=c =3;

​              int a = 3;   a+=5;等同运算a=a+5;

l  思考：

​              short s = 3;

s=s+2;

s+=2;   

有什么区别？     

l  注1：比较运算符的结果都是boolean型，也就是要么是true，要么是false。

l  注2：比较运算符“==”不能误写成“=” 。

 

### 逻辑运算符

 

### 位运算符

相当于进位或退位

 

 

00000001

### 三元运算符

l  格式

- (条件表达式)?表达式1：表达式2；
- 如果条件为true，运算后的结果是表达式1；
- 如果条件为false，运算后的结果是表达式2；

l  示例：

- 获取两个数中大数。
- int x=3,y=4,z;
- z = (x>y)?x:y;//z变量存储的就是两个数的大数。

 

## 程序流程控制

### 判断结构if: 

if(条件表达式)

{

执行语句:

}

else if(条件表达式)

{

​        执行语句:

}

else

​       {

​              执行语句:

}

If语句特点: a.每一种格式都是单条语句.

1. 第二种格式与三元运算符的区别:三元运算符运算完有值出现.好处是:可以写在其他表达式中.
2. 条件表达式无论写成什么样子，只看最终的结构是否是true 或者 false;

 

### 选择结构switch:

switch语句

格式：

switch(表达式)

{

​       case 取值1:

​              执行语句；

​              break；

​       case 取值2:

​              执行语句；

​              break；

​       …...

​       default:

​              执行语句；

​              break；

}

switch语句特点：

​     a,switch语句选择的类型只有四种：byte，short，int ， char。

​       b,case之间与default没有顺序。先执行第一个case，没有匹配的case执行default。

​       c,结束switch语句的两种情况：遇到break，执行到switch语句结束。

​       d,如果匹配的case或者default没有对应的break，那么程序会继续向下执行，运行可以执行的语句，直到遇到break或者switch结尾结束。

 

### 循环结构while for do..while:

while语句格式：

while(条件表达式)

{

​       执行语句；

}

 

 

do while语句格式：

do

{

​       执行语句；

}while(条件表达式);

do while特点：

是条件无论是否满足，

循环体至少执行一次。

 

For语句格式：

for(初始化表达式；循环条件表达式；循环后的操作表达式)

{

​       执行语句；(循环体)

}

注：

​          a,for里面的连个表达式运行的顺序，初始化表达式只读一次，判断循环条件，为真就执行循环体，然后再执行循环后的操作表达式，接着继续判断循环条件，重复找个过程，直到条件不满足为止。

​          b,while与for可以互换，区别在于for为了循环而定义的变量在for循环结束就是在内存中释放。而while循环使用的变量在循环结束后还可以继续使用。

​          c,最简单无限循环格式：while(true) , for(;;),无限循环存在的原因是并不知道循环多少次，而是根据某些条件，来控制循环。

 

### 其他流程控制语句break,continue

**break(跳出)，  continue(继续)**

break语句：应用范围：选择结构和循环结构。

continue语句：应用于循环结构。

注：

a,这两个语句离开应用范围，存在是没有意义的。

b,这个两个语句单独存在下面都不可以有语句，因为执行不到。

c,continue语句是结束本次循环继续下次循环。

d,标号的出现，可以让这两个语句作用于指定的范围。

 

## 函数

### 函数的定义

l  什么是函数？

- 函数就是定义在类中的具有特定功能的一段独立小程序。
- 函数也称为方法。

l  函数的格式：

- 修饰符 返回值类型 函数名(参数类型 形式参数1，参数类型 形式参数2，...)

​       {

​                     执行语句;

​                     return 返回值;

​       }

​       返回值类型：函数运行后的结果的数据类型。

​       参数类型：是形式参数的数据类型。

​       形式参数：是一个变量，用于存储调用函数时传递给函数的实际参数。

​       实际参数：传递给形式参数的具体数值。

​       return：用于结束函数。

​       返回值：该函数运算后的结果，该结果会返回给调用者。

### 函数的特点

l  定义函数可以将功能代码进行封装

l  便于对该功能进行复用

l  函数只有被调用才会被执行

l  函数的出现提高了代码的复用性

l  对于函数没有具体返回值的情况，返回值类型用关键字void表示，那么该函数中的return语句如果在最后一行可以省略不写。

### 函数的应用

l  两个明确

- 明确要定义的功能最后的结果是什么？
- 明确在定义该功能的过程中，是否需要未知内容参与运算

l  示例：

- 需求：定义一个功能，可以实现两个整数的加法运算。
- 分析：
- 该功能的运算结果是什么？两个数的和，也是一个整数(int)
- 在实现该功能的过程中是否有未知内容参与运算？加数和被加数是不确定的。(两个参数int，int)
- 代码：

​              int  getSum(int x,int y)

​              {

​                     return x+y;

​              }

### 函数的重载(overload)

 

## 数组

元素类型[] 数组名 = new 元素类型[元素个数或数组长度];

示例：int[] arr = new int[5];

元素类型[] 数组名 = new 元素类型[]{元素，元素，……};

int[] arr = new int[]{3,5,1,7};

int[] arr = {3,5,1,7};

 

Java程序在运行时，需要在内存中的分配空间。为了提高运算效率，有对空间进行了不同区域的划分，因为每一片区域都有特定的处理数据方式和内存管理方式。

l  用于存储局部变量，当数据使用完，所占空间会自动释放。

l  数组和对象，通过new建立的实例都存放在堆内存中。

l   每一个实体都有内存地址值

l   实体中的变量都有默认初始化值

l   实体不在被使用，会在不确定的时间内被垃圾回收器回收

 

数组脚标越界异常(ArrayIndexOutOfBoundsException)

int[] arr = new int[2];

System.out.println(arr[3]);

**访问到了数组中的不存在的脚标时发生。**

 

空指针异常(NullPointerException)

int[] arr = null;

System.out.println(arr[0])**;**

**arr引用没有指向实体，却在操作实体中的元素时。**

 

 

 

 

# 3月14日课程笔记

## 面向对象

栈:先进后出,一个快速的存储空间,栈的空间在CPU的二级缓存里面

Main( )函数  int a,b;

a = 1 ;  b =2 ;   

堆:在内存里面,当分配空间的时候,系统分配一个空间给定义的变量,然后由栈中的内存地址,指向堆中的首地址.

 

 return (a >= b) ? a : b;

 

 

## 继承类

旅行社里面大部分员工，做计调的，基本上每个人的功能差不多。

继承：保持原有功能，在此基础上开拓出新的功能。

多个类中存在相同属性和行为时，将这些内容抽取到单独一个类中，name这个类无需再定义这些属性和行为，只要继承那个类即可。

子类可以直接访问父类中的费私有的属性和行为。

继承的出现提高了代码的复用性。

减少了代码的冗余

继承的出现让类与类之间产生了关系，提供了多态的前提。

Java只支持单继承，不支持多继承。

Java支持多层继承(继承体系)

 

注意：不要为了获取某个功能而去继承。

 

如果是继承中，有参数的构造函数必须重新写。

 

访问一个成员变量时，

1. 首先在局部范围找
2. 继续在成员位置找
3. 最后在父类找
4. 找不到报错

 

父类中的私有方法不可以被重写。

子类方法的访问权限一定要大于等于父类方法访问权限

静态的方法只能被静态方法重写。这个其实不能算对象的关系。

覆盖的应用。

多态的意义：

把不同的子类对象都当作父类来看，可以屏蔽不同子类对象之间的差异，写出通用的代码，做出通用的编程，以适应需求的不断变化。   赋值之后，父对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。也就是说，父亲的行为像儿子，而不是儿子的行为像父亲。   举个例子：从一个基类中派生，响应一个虚命令，产生不同的结果。   比如从某个基类继承出多个对象，其基类有一个虚方法Tdoit，然后其子类也有这个方法，但行为不同，然后这些子对象中的任何一个可以赋给其基类的对象，这样其基类的对象就可以执行不同的操作了。实际上你是在通过其基类来访问其子对象的，你要做的就是一个赋值操作。   使用继承性的结果就是可以创建一个类的家族，在认识这个类的家族时，就是把导出类的对象当作基类的的对象，这种认识又叫作upcasting。这样认识的重要性在于：我们可以只针对基类写出一段程序，但它可以适应于这个类的家族，因为编译器会自动就找出合适的对象来执行操作。这种现象又称为多态性。而实现多态性的手段又叫称动态绑定(dynamic binding)。   简单的说，建立一个父类的变量，它的内容可以是这个父类的，也可以是它的子类的,当子类拥有和父类同样的函数，当使用这个变量调用这个函数的时候，定义这个变量的类（也就是父类）里的同名函数将被调用，当在父类里的这个函数前加virtual关键字，那么子类的同名函数将被调用。

 

 

有两个好处：

\1. 应用程序不必为每一个派生类编写功能调用，只需要对抽象基类进行处理即可。大大提高程序的可复用性。//继承

\2. 派生类的功能可以被基类的方法或引用变量所调用，这叫向后兼容，可以提高可扩充性和可维护性。//多态的真正作用，

 

 

行程计划

费用

预付费用

 

 

# 3月18日预习笔记

## 行业分享知识：

发现问题，解决问题。

问题：你为什么喜欢数据分析

不要回答，我感兴趣

你要给他讲故事，举例子，你实际生活中的遇到的问题。

你感兴趣的点。一定要跟HR讲故事。

 

 

## Hadoop入门

### 一：大数据时代的数据特点：

1、数据量大

n  根据IDC作出的估测，数据一直都在以每年50%的速度增长，也就是说每两年就增长一倍（大数据摩尔定律）

n  人类在最近两年产生的数据量相当于之前产生的全部数据量

n  预计到2020年，全球将总共拥有35ZB的数据量，相较于2010年，数据量将增长近30倍

2、数据类型繁多

n  大数据是由结构化和非结构化数据组成的

10%的结构化数据，存储在数据库中

90%的非结构化数据，它们与人类信息密切相关

3、价值取决于处理速度

p  从数据的生成到消耗，时间窗口非常小，可用于生成决策的时间非常少

p  1秒定律：这一点也是和传统的数据挖掘技术有着本质的不同

4、数据本身价值密度低

**价值密度低，整体价值高**

​       以视频为例，连续不间断监控过程中，可能有用的数据仅仅有一两秒，但是具有很高的商业价值

 

### 二：大数据关键技术：

1、数据采集

利用ETL工具将分布的、异构数据源中的数据如关系数据、平面数据文件等，抽取到临时中间层后进行清洗、转换、集成，最后加载到数据仓库或数据集市中，成为联机分析处理、数据挖掘的基础；或者也可以把实时采集的数据作为流计算系统的输入，进行实时处理分析

2、数据存储和管理

利用分布式文件系统、数据仓库、关系数据库、NoSQL数据库、云数据库等，实现对结构化、半结构化和非结构化海量数据的存储和管理

3、数据处理与分析

利用分布式并行编程模型和计算框架，结合机器学习和数据挖掘算法，实现对海量数据的处理和分析；对分析结果进行可视化呈现，帮助人们更好地理解数据、分析数据

4、在从大数据中挖掘潜在的巨大商业价值和学术价值的同时，构建隐私数据保护体系和数据安全体系，有效保护个人隐私和数据安全

 

 

### 三、大数据技术框架

 

**按照计算相应速度划分的计算模式**

### 四、Hadoop简介

Hadoop是Apache软件基金会旗下的一个开源分布式计算平台，为用户提供了系统底层细节透明的分布式基础架构

Hadoop是基于Java语言开发的，具有很好的跨平台特性，并且可以部署在廉价的计算机集群中

Hadoop的核心是分布式文件系统HDFS（Hadoop Distributed File System）和MapReduce

Hadoop被公认为行业大数据标准开源软件，在分布式环境下提供了海量数据的处理能力

几乎所有主流厂商都围绕Hadoop提供开发工具、开源软件、商业化工具和技术服务，如谷歌、雅虎、微软、思科、淘宝等，都支持Hadoop

Hadoop最初是由Apache Lucene项目的创始人Doug Cutting开发的文本搜索库。Hadoop源自始于2002年的Apache Nutch项目——一个开源的网络搜索引擎并且也是Lucene项目的一部分。

 在2004年，Nutch项目也模仿GFS开发了自己的分布式文件系统NDFS（Nutch Distributed File System），也就是HDFS的前身

 2004年，谷歌公司又发表了另一篇具有深远影响的论文，阐述了MapReduce分布式编程思想

 2005年，Nutch开源实现了谷歌的MapReduce

到了2006年2月，Nutch中的NDFS和MapReduce开始独立出来，成为Lucene项目的一个子项目，称为Hadoop，同时，Doug Cutting加盟雅虎

 2008年1月，Hadoop正式成为Apache顶级项目，Hadoop也逐渐开始被雅虎之外的其他公司使用

 2008年4月，Hadoop打破世界纪录，成为最快排序1TB数据的系统，它采用一个由910个节点构成的集群进行运算，排序时间只用了209秒

在2009年5月，Hadoop更是把1TB数据排序时间缩短到62秒。Hadoop从此名声大震，迅速发展成为大数据时代最具影响力的开源分布式开发平台，并成为事实上的大数据处理标准。

Hadoop凭借其突出的优势，已经在各个领域得到了广泛的应用，而互联网领域是其应用的主阵地

 2007年，雅虎在Sunnyvale总部建立了M45——一个包含了4000个处理器和1.5PB容量的Hadoop集群系统

 Facebook作为全球知名的社交网站，Hadoop是非常理想的选择，Facebook主要将Hadoop平台用于日志处理、推荐系统和数据仓库等方面

 国内采用Hadoop的公司主要有百度、淘宝、网易、华为、中国移动等，其中，淘宝的Hadoop集群比较大

 

### 五、Hadoop的特性

​       Hadoop是一个能够对大量数据进行分布式处理的软件框架，并且是以一种可靠、高效、可伸缩的方式进行处理的，它具有以下几个方面的特性：

-    高可靠性
-    高效性
-    高可扩展性
-    高容错性
-    成本低
-    运行在Linux平台上
-    支持多种编程语言

Hadoop项目结构

六、企业级集群硬件配置

在集群中，大部分的机器设备是作为Datanode和TaskTracker工作的Datanode/TaskTracker的硬件规格可以采用以下方案：

4个磁盘驱动器（单盘1-2T），支持JBOD(Just a Bunch Of Disks，磁盘簇)

2个4核CPU,至少2-2.5GHz

16-24GB内存

千兆以太网

 

NameNode提供整个HDFS文件系统的NameSpace(命名空间)管理、块管理等所有服务，因此需要更多的RAM，与集群中的数据块数量相对应，并且需要优化RAM的内存通道带宽，采用双通道或三通道以上内存。硬件规格可以采用以下方案：

8-12个磁盘驱动器（单盘1-2T）

2个4核/8核CPU

16-72GB内存

千兆/万兆以太网

SecondaryNameNode在小型集群中可以和NameNode共用一台机器，较大的群集可以采用与NameNode相同的硬件

 

集群规模

Hadoop集群规模可大可小，初始时，可以从一个较小规模的集群开始，比如包含10个节点，然后，规模随着存储器和计算需求的扩大而扩大

如果数据每周增大1TB，并且有三个HDFS副本，然后每周需要一个额外的3TB作为原始数据存储。要允许一些中间文件和日志（假定30%）的空间，由此，可以算出每周大约需要增加一台新机器。存储两年数据的集群，大约需要100台机器

对于一个小的集群，名称节点（NameNode）和JobTracker运行在单个节点上，通常是可以接受的。但是，随着集群和存储在HDFS中的文件数量的增加，名称节点需要更多的主存，这时，名称节点和JobTracker就需要运行在不同的节点上

第二名称节点（SecondaryNameNode）会和名称节点可以运行在相同的机器上，但是，由于第二名称节点和名称节点几乎具有相同的主存需求，因此，二者最好运行在不同节点上

 

集群网络拓扑

普通的Hadoop集群结构由一个两阶网络构成

每个机架（Rack）有30-40个服务器，配置一个1GB的交换机，并向上传输到一个核心交换机或者路由器（1GB或以上）

在相同的机架中的节点间的带宽的总和，要大于不同机架间的节点间的带宽总和

 

 

### 六、HDFS简介

HDFS要实现以下目标：

- **兼容廉价的硬件设备**
- **流数据读写**
- **大数据集**
- **简单的文件模型**
- **强大的跨平台兼容性**

HDFS特殊的设计，在实现上述优良特性的同时，也使得自身具有一些应用局限性，主要包括以下几个方面：

**不适合低延迟数据访问**

- **无法高效存储大量小文件**
- **不支持多用户写入及任意修改文件**

**HDFS核心概念：块**

HDFS默认一个块128MB，一个文件被分成多个块，以块作为存储单位

块的大小远远大于普通文件系统，可以最小化寻址开销

HDFS采用抽象的块概念可以带来以下几个明显的好处：

​        **●  支持大规模文件存储**：文件以块为单位进行存储，一个大规模文件可以被分拆成若干个文件块，不同的文件块可以被分发到不同的节点上，因此，一个文件的大小不会受到单个节点的存储容量的限制，可以远远大于网络中任意节点的存储容量

​        **●    简化系统设计**：首先，大大简化了存储管理，因为文件块大小是固定的，这样就可以很容易计算出一个节点可以存储多少文件块；其次，方便了元数据的管理，元数据不需要和文件块一起存储，可以由其他系统负责管理元数据

​        **●    适合数据备份**：每个文件块都可以冗余存储到多个节点上，大大提高了系统的容错性和可用性

 

在HDFS中，名称节点（NameNode）负责管理分布式文件系统的命名空间（Namespace），保存了两个核心的数据结构，即FsImage和EditLog

- FsImage用于维护文件系统树以及文件树中所有的文件和文件夹的元数据
- 操作日志文件EditLog中记录了所有针对文件的创建、删除、重命名等操作

名称节点记录了每个文件中各个块所在的数据节点的位置信息

**FsImage文件**

 

FsImage文件包含文件系统中所有目录和文件inode的序列化形式。每个inode是一个文件或目录的元数据的内部表示，并包含此类信息：文件的复制等级、修改和访问时间、访问权限、块大小以及组成文件的块。对于目录，则存储修改时间、权限和配额元数据

FsImage文件没有记录块存储在哪个数据节点。而是由名称节点把这些映射保留在内存中，当数据节点加入HDFS集群时，数据节点会把自己所包含的块列表告知给名称节点，此后会定期执行这种告知操作，以确保名称节点的块映射是最新的。

**名称节点的启动**

在名称节点启动的时候，它会将FsImage文件中的内容加载到内存中，之后再执行EditLog文件中的各项操作，使得内存中的元数据和实际的同步，存在内存中的元数据支持客户端的读操作。

一旦在内存中成功建立文件系统元数据的映射，则创建一个新的FsImage文件和一个空的EditLog文件

名称节点起来之后，HDFS中的更新操作会重新写到EditLog文件中，因为FsImage文件一般都很大（GB级别的很常见），如果所有的更新操作都往FsImage文件中添加，这样会导致系统运行的十分缓慢，但是，如果往EditLog文件里面写就不会这样，因为EditLog 要小很多。每次执行写操作之后，且在向客户端发送成功代码之前，edits文件都需要同步更新

**名称节点运行期间EditLog不断变大的问题**

在名称节点运行期间，HDFS的所有更新操作都是直接写到EditLog中，久而久之， EditLog文件将会变得很大

虽然这对名称节点运行时候是没有什么明显影响的，但是，当名称节点重启的时候，名称节点需要先将FsImage里面的所有内容映像到内存中，然后再一条一条地执行EditLog中的记录，当EditLog文件非常大的时候，会导致名称节点启动操作非常慢，而在这段时间内HDFS系统处于安全模式，一直无法对外提供写操作，影响了用户的使用

 

如何解决？答案是：SecondaryNameNode第二名称节点

 

**第二名称节点**是HDFS架构中的一个组成部分，它是用来保存名称节点中对HDFS 元数据信息的备份，并减少名称节点重启的时间。SecondaryNameNode一般是单独运行在一台机器上

SecondaryNameNode的工作情况：

（1）SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件，暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成，上层写日志的函数完全感觉不到差别；

　　（2）SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文件，并下载到本地的相应目录下；

　　（3）SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件中的各项更新操作，使得内存中的FsImage保持最新；这个过程就是EditLog和FsImage文件合并；

　　（4）SecondaryNameNode执行完（3）操作之后，会通过post方式将新的FsImage文件发送到NameNode节点上

　　（5）NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件，同时将edit.new替换EditLog文件，通过这个过程EditLog就变小了

**数据节点**（DataNode）

数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，会根据客户端或者是名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块的列表

每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中

### 七、HDFS体系结构概述

​        HDFS采用了主从（Master/Slave）结构模型，一个HDFS集群包括一个名称节点（NameNode）和若干个数据节点（DataNode）（如图3-4所示）。名称节点作为中心服务器，负责管理文件系统的命名空间及客户端对文件的访问。集群中的数据节点一般是一个节点运行一个数据节点进程，负责处理文件系统客户端的读/写请求，在名称节点的统一调度下进行数据块的创建、删除和复制等操作。每个数据节点的数据实际上是保存在本地Linux文件系统中的。

**HDFS命名空间管理**

HDFS的命名空间包含目录、文件和块

在HDFS1.0体系结构中，在整个HDFS集群中只有一个命名空间，并且只有唯一一个名称节点，该节点负责对这个命名空间进行管理，而在HDFS2.0体系中，通过增加SecondaryNameNode，即增加了分布式文件系统的稳定性，也增加了系统实际上的有效性。

HDFS使用的是传统的分级文件体系，因此，用户可以像使用普通文件系统一样，创建、删除目录和文件，在目录间转移文件，重命名文件等

**通信协议**

HDFS是一个部署在集群上的分布式文件系统，因此，很多数据需要通过网络进行传输

所有的HDFS通信协议都是构建在TCP/IP协议基础之上的

客户端通过一个可配置的端口向名称节点主动发起TCP连接，并使用客户端协议与名称节点进行交互

名称节点和数据节点之间则使用数据节点协议进行交互

客户端与数据节点的交互是通过RPC（Remote Procedure Call）来实现的。在设计上，名称节点不会主动发起RPC，而是响应来自客户端和数据节点的RPC请求

**客户端**

客户端是用户操作HDFS最常用的方式，HDFS在部署时都提供了客户端

HDFS客户端是一个库，暴露了HDFS文件系统接口，这些接口隐藏了HDFS实现中的大部分复杂性

严格来说，客户端并不算是HDFS的一部分

客户端可以支持打开、读取、写入等常见的操作，并且提供了类似Shell的命令行方式来访问HDFS中的数据

此外，HDFS也提供了Java API，作为应用程序访问文件系统的客户端编程接口

**冗余数据保存**

作为一个分布式文件系统，为了保证系统的容错性和可用性，HDFS采用了多副本方式对数据进行冗余存储，通常一个数据块的多个副本会被分布到不同的数据节点上，如图3-5所示，数据块1被分别存放到数据节点A和C上，数据块2被存放在数据节点A和B上。这种多副本方式具有以下几个优点：

​    （1）**加快数据传输速度**

​    （2）**容易检查数据错误**

​    （3）**保证数据可靠性**

**数据存取策略**

**数据存放**

第一个副本：放置在上传文件的数据节点；如果是集群外提交，则随机挑选一台磁盘不太满、CPU不太忙的节点

第二个副本：放置在与第一个副本不同的机架的节点上

第三个副本：与第一个副本相同机架的其他节点上

更多副本：随机节点

Block的副本放置策略

**2. 数据读取**

HDFS提供了一个API可以确定一个数据节点所属的机架ID，客户端也可以调用API获取自己所属的机架ID

当客户端读取数据时，从名称节点获得数据块不同副本的存放位置列表，列表中包含了副本所在的数据节点，可以调用API来确定客户端和这些数据节点所属的机架ID，当发现某个数据块副本对应的机架ID和客户端对应的机架ID相同时，就优先选择该副本读取数据，如果没有发现，就随机选择一个副本读取数据

**数据错误与恢复**

​        HDFS具有较高的容错性，可以兼容廉价的硬件，它把硬件出错看作一种常态，而不是异常，并设计了相应的机制检测数据错误和进行自动恢复，主要包括以下几种情形：名称节点出错、数据节点出错和数据出错。

**1. 名称节点出错**

​        名称节点保存了所有的元数据信息，其中，最核心的两大数据结构是FsImage和Editlog，如果这两个文件发生损坏，那么整个HDFS实例将失效。因此，HDFS设置了备份机制，把这些核心文件同步复制到备份服务器SecondaryNameNode上。当名称节点出错时，就可以根据备份服务器SecondaryNameNode中的FsImage和Editlog数据进行恢复。

**2. 数据节点出错**

每个数据节点会定期向名称节点发送“心跳”信息，向名称节点报告自己的状态

当数据节点发生故障，或者网络发生断网时，名称节点就无法收到来自一些数据节点的心跳信息，这时，这些数据节点就会被标记为“宕机”，节点上面的所有数据都会被标记为“不可读”，名称节点不会再给它们发送任何I/O请求

这时，有可能出现一种情形，即由于一些数据节点的不可用，会导致一些数据块的副本数量小于冗余因子

名称节点会定期检查这种情况，一旦发现某个数据块的副本数量小于冗余因子，就会启动数据冗余复制，为它生成新的副本

HDFS和其它分布式文件系统的最大区别就是可以调整冗余数据的位置

**3. 数据出错**

网络传输和磁盘错误等因素，都会造成数据错误

客户端在读取到数据后，会采用md5和sha1对数据块进行校验，以确定读取到正确的数据

在文件被创建时，客户端就会对每一个文件块进行信息摘录，并把这些信息写入到同一个路径的隐藏文件里面

当客户端读取文件的时候，会先读取该信息文件，然后，利用该信息文件对每个读取的数据块进行校验，如果校验出错，客户端就会请求到另外一个数据节点读取该文件块，并且向名称节点报告这个文件块有错误，名称节点会定期检查并且重新复制这个块

**读数据的过程**

### 八、HDFS数据读写过程

FileSystem是一个通用文件系统的抽象基类，可以被分布式文件系统继承，所有可能使用Hadoop文件系统的代码，都要使用这个类

Hadoop为FileSystem这个抽象类提供了多种具体实现

DistributedFileSystem就是FileSystem在HDFS文件系统中的具体实现

FileSystem的open()方法返回的是一个输入流FSDataInputStream对象，在HDFS文件系统中，具体的输入流就是DFSInputStream；FileSystem中的create()方法返回的是一个输出流FSDataOutputStream对象，在HDFS文件系统中，具体的输出流就是DFSOutputStream。

Configuration conf = new Configuration();

FileSystem fs = FileSystem.get(conf);

FSDataInputStream in = fs.open(new Path(uri));

FSDataOutputStream out = fs.create(new Path(uri));

 

备注：创建一个Configuration对象时，其构造方法会默认加载工程项目下两个配置文件，分别是hdfs-site.xml以及core-site.xml，这两个文件中会有访问HDFS所需的参数值，主要是fs.defaultFS，指定了HDFS的地址（比如hdfs://localhost:9000），有了这个地址客户端就可以通过这个地址访问HDFS了

注：1.使用HDFS前必须先启动Hadoop集群，可使用命令行 start-all.sh

​       2.在配置好Hadoop集群之后，可以通过浏览器登录      “http://[*NameNodeIP*]:50070”访问HDFS文件系统

 

 

# 3月19日 课程笔记

## 课堂内容记录

拥有什么神经网络算法的方法，我们目前称为机器学习。
机器学习，算法简单，但是只要数据框够,肯定能有一个算法。

 

我们发现神经网络能达到的预期效果，并不理想，由此数据科学发展陷入低谷。

我们发现企业并不太需要这种复杂的神经算法，可解释性非常差，用这么复杂的算法解决看起来这么高大上的东西。

 

主要就是使用一些BI工具，数据专员的工作，powerBI，当你面临这么一个行业选择的时候，可能你所做的是商业智能，以实际应用为基础，简化了算法的使用。

 

分布式计算工具：对海量数据进行分析的基础工具。

以是否能单机运行为标准的。

一些迭代的相关结构分析的。

你只要单机运行不了的，你就必须上我们分布式工具。

 

### 算法研发--大数据架构--算法执行

1、 大数据架构：我们的分布式集群软件工具，搭建和维护我们的计算机架构，有一批人专门的负责搭建集群，对性能进行优化，或者叫做大数据。

2、 算法执行：也就是我们所说的数据分析，更加侧重于利用现在已经有的算法，通过调参等方法进行算法优化，结合实际业务情景进行分析。

3、 算法研发：更加侧重于利用数学底层原理，针对现有业务需求，进行算法设计与研发。

很多公司都不区分的

算法执行：第一，这些算计基本的用法和原理，核心是会用不同的工具，使用工具把算法给落地。

第一个层面是单机执行的工具，毫无以为以python为主

第二个层面是分布式集群的工具。

我把python研究透了，至少能比excel强吧，R语言也是可以取代的。

Spark最重要的，没有之一。

 

Excel Mysql重要不重要，也重要，因为它是基础工具的。

 

Excel知道简单的导入导出，写个函数就行了。

 

R语言是S语言的开源实现，统计分析和数据可视化语言，R语言底层封装程度较高，算法执行和可视化功能而言比"可执行的伪代码"Python要更加简洁、更加容易上手，且社区活跃，其最具特色的(Package)功能似的其能以调用的方式轻松实现最前沿的算法

 

SAS最为最顶级的商业统计分析软件,提供了极其精准底层算法运行模型与简单的变成操作语言,对于大多数中小型数据公司,并不多见.

 

Python作为当下最火热的机器学习语言，拥有强大且规整的算法库、活跃的社区支持、以及简单的语法结构（被誉为“可执行的伪代码”），尤其近几年随着数据挖掘、机器学习及深度学习的持续升温，Python使用人数呈爆发式增长，同时也是大多数数据分析岗位要求必备的编程语言，也是CDA课程后续将进行长期深入介绍讲解的编程语言。

 

Scala语言可以说是随着分布式计算框架Spark的兴起而被广泛关注的一门语言，也是大数据分析领域非常具有潜力的一门语言。由于Scala由JAVA发展而来，因此本身编程难度要大于Python，但是随着Spark逐渐成为分布式内存计算的行业标准，以及MLLib算法库的不断完善，Scala也将被更加广泛的应用于大数据分析的各个领域。

 

从本质上而言，JAVA并不是一门特别适合编写算法与进行数据处理的语言，对于数据分析师而言，可能接触JAVA最多的场景就是在大数据领域编写MapReduce程序时使用，除此之外使用JAVA进行数据分析的场景并不多见。

 

至今为止，仍然有很多顶级的算法工程师采用C语言或C++进行算法研发，他们拥有深厚的数学功底以及丰富的C或C++使用经验，是数据分析师中技术最顶尖的人群。但对于大多数数据分析工作而言，采用C或C++来进行数据分析仍然是一件非常奢侈的事情。

 

数据分析会不会被人工智能给干掉了呢，人工智能，如果仅仅是和规则相关，和业务不想管的工作，会迅速的被人工智能给干掉。

 

数据分析需要大量的人工的介入的。实实在在的了解客户的需求。

人工智能没有办法达到的，数据分析师的行业寿命可能会比较长的。

 

Linux操作，能用命令行的尽量用命令行操作，这样以后比较方便。

 

## Linux基本操作命令：

在课件中: 1.虚拟机环境准备与Hadoop基本安装.pdt 文件有详细记录

rm –rf 文件或者文件夹   都可以删除

 

bin 包含了二进制脚本文件夹

sbin 更高一级的脚本文件,服务怎么开启怎么关闭.

etc Hadoop配置文件所在目录，Hadoop生态中各组件都是通过修改配置文件来进行不同模式的运行

share Hadoop各模块编译后jar包所在目录

lib Hadoop对外的编程动态库和静态库，与include中的头文件配合使用

include包含对外的编程头文件

 

-C 代表输入安装命令

sudo tar -zxvf hadoop-2.2.0.tar.gz -C /usr/local

 

sudo chown -R hadoop ./Hadoop

把这个文件夹付权给这个目录

 

## centOS安装记录（不要尝试去记忆）：

查看IP：ip addr

 

 

# 3月20日 课堂笔记

Hadoop老师介绍

为什么要出现YARN

因为，hadoop发现它必须要兼容能够放入不同的组件，才能更好的和其他软件进行协作。

 

在中间插入资源管理框架独立出来,YARN,利用它能够更好的兼容其他spark之类的组件.

 

迭代是我们机器学习的最显著特征,上一次的输出结果作为下一次的参数,有大量的中间结果.

 

如果你把所有的中间结果都存到磁盘上去,这个速度太慢了.

 

Spark是DAG计算引擎,

而MapReduce就是这个,所有的运算都会转换成两种,一个是Map,一个是Reduce,计算模型非常简单.

它没有办法进行复杂的运算.

 

Flink是比较新出的计算工具,我们暂时还不会进行学习.

 

分布式的数据仓库：核心组件是Hive，不具备自身的存储系统，数据存储依然要依靠HDFS进行文件存储，而数据库是在底层之上有一层更高级的。

他的查询语言叫做HQL语言，很像我们的Mysql语言

 

Hive把sql语句翻译成MapReduce能听懂的，在HDFS中进行查询。

Hbase是一个典型的Nosql，它是直接作用在HDFS上，对它进行存储。

Zookeeper(分布式协调服务)，每个节点都必须要有一个

Ambari (一键安装部署工具)

 

Cloudera 商业开发Haoop版本的公司

Hortonworks 排名第二

 

商业版本,性能上要好的多,这些厂商在进行

仅限于一些中高层公司

最高的都是自己开发,而底层的自己组建团队开发,

 

Hortonworkers 完全免费,但是遇到问题你就得找他们来帮你解决了.

 

<http://192.168.152.128:8088>  hadoop的文件管理界面

<http://192.168.152.128:50070> hdfs的文件管理界面

 

复制文件到hdfs

hadoop fs -copyFromLocal /usr/local/hadoop/README.txt /user/root/test

强制复制

hadoop fs -copyFromLocal -f /usr/local/hadoop/README.txt /user/root/test

同时复制多个文件

hadoop fs -copyFromLocal /usr/local/hadoop/NOTICE.txt /usr/local/hadoop/NOTICE.txt

/user/root/test

 

 

CopyFromLocal   CopyToLoca

可以换成 -put  -get

 

 

### Zookeeper

log4j.properties  日志

zoo_sample.cfg  核心操作文件

 

 

# 3月21日课堂笔记

 

Chmod u+x –R ~/scala

u代表当前的管理者,  + 新增权限, - 删除权限  x 执行权限  给u管理者赋予 执行权限  w,r  写和读

 

也可以用数字代替这些符号

u

g

o

+

_

r --- 4

w---2

x  ---1

chmod   777  -R   ~/scala    

7代表(4+2+1) 赋予7代表有全部权限  4 只有读权限  2  写权限 

我需要赶快把另外一个系统的记忆清楚出去,已经对我产生了严重的影响.

 

 

## 常用启动命令

Hadoop

 

在etc/hosts 中编辑好 slave 的ip地址,就可以用hostname 来代替 ip的使用.

 

### JAVA_HOME

=/usr/lib/jdk1.7.0_51

 

\#Java

export JAVA_HOME=/usr/lib/jdk1.7.0_51

export PATH=$PATH:$JAVA_HOME/bin

\#Hadoop

export HADOOP_HOME=/home/hduser/hadoop

export PATH=$PATH:$HADOOP_HOME/bin

export PATH=$PATH:$HADOOP_HOME/sbin

export HADOOP_MAPRED_HOME=$HADOOP_HOME

export HADOOP_COMMON_HOME=$HADOOP_HOME

export HADOOP_HDFS_HOME=$HADOOP_HOME

export YARN_HOME=$HADOOP_HOME

export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native

export HADOOP_OPTS="-DJava.library.path=$HADOOP_HOME/lib"

export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH

\#zookeeper

export ZOOKEEPER_HOME=/home/hduser/zookeeper

export PATH=$PATH:$ZOOKEEPER_HOME/bin

 

\#Hbase

export HBASE_HOME=/home/hduser/hbase

export PATH=$PATH:$HBASE_HOME/bin

\#Scala

export SCALA_HOME=/home/hduser/scala

export PATH=$PATH:$SCALA_HOME/bin

\#spark

export SPARK_HOME=/home/hduser/spark

export PATH=$PATH:$SPARK_HOME/bin

\#mahout

export MAHOUT_HOME=/home/hduser/mahout

export PATH=$PATH:$MAHOUT_HOME/bin

 

 

启动过程中常见错误

1. 配置文件出错

2.tmp文件没有删除

3.hdfs datanote –format

4.hdfs namenote –format

 

stop-hbase.sh # 先关闭HBase

zkServer.sh stop # 关闭ZooKeeper命令需要在各节点执行

/usr/local/spark/sbin/stop-all.sh # 关闭Spark

 

scp ~/.ssh/id_rsa.pub hduser@slave1:~/下载

 

sudo vim /etc/network/interfaces

 

sudo vim /etc/hostname

sudo vim /etc/hosts

start-all.sh

~/spark/sbin/start-all.sh

zkServer.sh start

start-hbase.sh

~/zeppelin/bin/zeppelin-daemon.sh start

~/zeppelin/bin/zeppelin-daemon.sh stop

stop-hbase.sh

zkServer.sh stop

~/spark/sbin/stop-all.sh

stop-all.sh

 

容易出现的问题:

本地回环:sudo vim /etc/hosts

中的地址

 

**在老师课程中的 查看日志方法,回头重新看一下课程视频**

 

**4266 NameNode**

**4606 ResourceManager**

**5134 QuorumPeerMain**

**5772 Jps**

**4463 SecondaryNameNode**

**4953 Master**

**5478 Hmaster**

 

**Hbase 分布式数据库,是一个典型的nosql,**

**Spark 分布式计算引擎**

**Mapreduce 功能比较单一**

**Hive,数据仓库工具,和我们的hbase有一个本质区别.**

**Hbase体量更大,**

**Hive只是一个数仓工具,提供了一个类sql语言叫做HQL,可以翻译成MapReduce程序,可以进一步作用于hdfs, 从而查询我们底层分布式系统文件的功能.**

**对用户来看,可能你会觉得在使用用途上没有太大的区别,最核心的在他们底层功能的不同.**

**而hive会简单许多,没有数据行数据列**

 

**#Hive**

**export HIVE_HOME=/usr/local/hive**

**export PATH=$PATH:$HIVE_HOME/bin**

 

 

**export HADOOP_COMMON_HOME=/usr/local/hadoop**

**export HADOOP_MAPRED_HOME=/usr/local/hadoop**

**export HBASE_HOME=/usr/local/hbase**

**export HIVE_HOME=/usr/local/hive**

**export ZOOCFGDIR=/usr/local/zookeeper**

 

 

# 3 月22日课堂笔记

HDFS四种api

hadoop fs    shell 命令行,简单,不可重复

web端  简单,点点点,给分析人员使用

java  程序调用,分析人员很少用

执行MapReduce 的 jar包

jar包:执行hadoop算法程序,对分析人员来说不是特别好用的.需要先写分析代码,再打包成jar包

 

hbase和hive都提供了shell接口

 

在shell中的命令不可重复,而且没办法和其他功能进行集成

也提供了开发的借口,给编程人员进行使用.

 

Spark: 大数据计算一站式解决方法,

Api接口:

1.Shell界面可以进行分析

1. 编程端口是一个scala的一个端口
2. 但scala是基于java编译的,因此也可以用java进行编译.

 

Sqoop:轻量级的组件ETL工具,

只能用命令行进行操作

 

### 各种api接口直接的区别:

学习过程中我们主要用的就是shell,但shell也有很多的问题,结果很难保留.

操作环境没有办法进行修改

 

我们想跑一个算法,这个结论如果你是把它打印到一个屏幕上,很难和其他部门进行交互.

 

数据分析师常用的接口

1. notebook形式的编程界面,只能基于shell端,从shell端接过来(jupyter notebook)
2. IDE : 可以接向我们的shell端

### Zeppelin介绍

Zeppelin,在一个notebook界面进行多种命令自由切换的功能, 从而集成了Hadoop中几乎全部主流组件的调度和使用功能, 即我

们能够在一个界面中完成从数据提取（HDFS、HBase）、数据探索（Spark SQL）、数据清洗（Hive）、数据分

析（Mahout、Spark MLlib）、结果可视化展示（Spark SQL）的全部流程，从而极大提高数据分析工作效率。

 

实现方法,在内部集成了很多翻译器,当外部人员使用代码的时候,zeppelin把程序划分成很多代码

 

 

### 大数据分析师需要掌握的工具

对于大数据分析师来说,可能要掌握两套工具,一套是单机运行的,一套是分布式:

单机:python, 有一个最常用的机器学习算法库

分布式:计算框架,或者是引擎—MapReduce,spark

刚开始进行分析,用的比较多的,先回调用算法.

分析工作用的比较多的,用算法,通常意义上的数据分析

算法从哪来?

很多时候我们要借助算法库

数据分析中常用的算法的实现

别人已经把算法写好了,放到里面,这些算法包含的类,函数方法,用户调用的方法,全都弄好了,写在算法库里面供别人用.

MapReduce:mahout算法库

Spark:Mllib算法库

 

### 不同代码实现当中,能不能相互借鉴呢

因为他们的,数据结构不同.

每种编程语言都有一套自身的数据结构,基于不同的数据结构,不同的编程语言可以创建出一套基于编程语言的计算方法.

封装出一套方法

向量,数组,矩阵

首先定义在数据结构之中,然后才有了各种方法对数据结构进行操作.

 

**因此算法是不通用的**

 

哪怕spark中的算法可以用java来写,但是他的编程规范也是不一样的.

一.   会调用他的数据结构

二.   会调用他的算法

 

 

接下来的安排,首先以MapReduce的框架,简单计算算法原理

稍微介绍算法原理

 

计算框架

算法原理

进行调用

如何进行计算

 

 

不同底层的算法引擎会导致能够运行在该算法引擎上的数据结构的不同.数据结构的不同又会直接影响它使用方法或者函数的不同.

Python,Scala,已经编译好的针对不同数据结构的算法库

mahout seqdumper -i /user/hduser/mk1/data/part-m-00000 –o /home/hduser/res

 

part-m-00000

 

# 3月25日 课堂笔记

## 孙老师:学习的重要性

以什么心态来学知识，很快会贬值，有效期间只有三到四年，如果三到四年你不学习你可能就要落伍.

学一个知识你不用特别精通，如果你想把一个学的特别精通再去工作，你是来不及的.

你只要知道去哪能找到这个东西.

算法不会更新换代.

 

对于数据分析师，对于代码效率高低，都不是特别重要.

把数据分析师的核心知识点，统计学知识点，

 

核心知识，

统计学知识，算法。

你学那些不容易贬值的东西。

 

把自己定位在什么样的定位上，你不是在学习东西，你是在研究东西。

公司很难安排人，教你去学习一个什么东西的。

学习路径，你要自己去找，找出一个相关的学习路径。

你要以一个研究者的学习心态，去学习东西。

 

先建立知识的框架：知识点之间是有相关性的，生成

 

 

## Hive数据仓库

Apache Hive建立在Hadoop上的开源数据仓库，它提供了SQL语言HQL，以便读取/写入和管理Hadoop中的海量数据集

 

只是充当了hadoop和用户之间的一个接口而已,真正的工作是建立

 

输出数据的是利用hive的端口，实际工作还是由hadoop来完成。

 

 

1、 用户连接hive，是通过命令行接口。

2、 用户发布的类sql语句命令，hive的优化器把类sql语言的命令翻译为MapReduce，然后将MapReduce发送给hadoop来执行

3、 Hadoop根据收到的MapReduce命令，调度资源，安排执行。

4、 Hadoop计算MapReduce的结果通过hive的接口传递给用户（交互式查询）

 

 

Hive能让原来用sql开发的数据，还能继续在hadoop分布式上面运行。

Hbase很难用sql语言来进行操作

 

### Hive的相似产品

Hive2.1支持内存计算

Spark sql 内存计算

CDH cloudear impala 类sql语言,支持内存计算,支持数据分析函数

 

### Hive的特性

支持使用HPL/SQL的储存过程语言

Hive Hybrid Procedural SQL On Hadoop 是一个在Hive上执行过程SQL的工具,它可以表达复杂的业务规则.

 

HPL/SQL具备以下特点:

支持使用变量、表达式、控制流声明、迭代来实现业务逻辑，支持使用异常处理程序和条件处理器来实现高级错误处理。

 

Hive设计源于bigtable，慎重使用多表查询。

 

使用SQL-on-hadoop更加动态，即：支持使用高级表达式、各种内置函数，基于用户配置、现在查询的结果、来自文件或非Hadoop数据源的数据，即时动态地生成SQL条件。

 

 

### Hive具体操作命令:

有两个hive;

Mysql中建立的hive,里面储存27-57张表,这个hive数据库里储存的是hive的元数据.

这些元数据是hive是hive 中定义对象的结构信息.例如你在hadoop的hive上建立一个表,那么表的名字,表的列名,表的存储位置/数据类型等信息就存储在这hive数据库中的表内。如果这数据库被删除，那么hadoop中hive数据仓库中定义的对象，元数据消失、定义就不见了—表就查询不到了，但是数据还在hadoop上。

 

Hadoop上的hive，这是一个nosql的数据仓库，文件分布式存储在hdfs上。

 

### 外部表

多种数据库中都有这个概念。

关系型数据库：外部表是存储在mysql数据库之外，直接存储在文件系统上的表结构。它有可能就是一个简单的txt文本文件，并没有mysql专属的数据结构。

数据不能被更改，也不能增加，也不能随机查询，只能全表查询。

 

这种外部表只是作为外部临时的数据来源，并不是数据库的主要结构。

 

Hive外部表：一类外部表是指数据存储在HDFS之外的操作系统文件系统上。在主机本地，为了方便临时使用，建立一个可以hive支持查询它的对象。就像mysql等的外部表。

 

另一类外部表，hive定义表结构的时候，外部表存储的路径并不是本地的HDFS文件系统，而是远程的另外一个HDFS文件系统。这个应用可以用于在两个大数据平台之间传递数据。

 

 

Hive操作：

在大数据环境下，我们获取数据的速率是不一样的，可能8小时获取一万个文件，每个文件1GB

这时候，从多台服务器获取的数据能够在长时间范围内方便的加入到hive数据仓库中，准备用来做数据分析，这很重要。这种情况就可以使用例如flume，对远程服务器的目录进行操作。将数据随时导入到HDFS，同时就进入了hive。数据进入HDFS是并行的。

 

更改表的字段名

通常在创建系统的时候指定，之后很少更改

各种业务可能会引用字段，变更容易造成其他相关的业务不可用。

有变更字段名称需求时，通常可以新建表，将旧表中的数据迁移到新表中。这样可能更有效率。也能提高数据库表的查询效率。

 

Hive中视图的用法与mysql中视图的用法是一样的，它也是一个定义，没有实体。

Show 显示对象

 

Describe invites;

 

 

安装到mysql,hive和mysql的链接还没用弄,到这一步了.

 

 

## Hive操作命令自己总结

查看表的分区: show partitions t3;

为表新建一个分区: alter table t3 add partition(story = 'shelock');

建立新表:create table t1(id int,name string,tedian string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;

建立分区表

 

统计单词数量

create table word_count as

select word, count(1) as count from

(select explode(split(value,' '))as word from qq1) w

group by word

order by word;

 

select uid,substr(tm,1,8),count(uid)

from search

where dt='20150212'

group by uid,substr(tm,1,8);

 

 

# 4月11日 课堂笔记

## 1.scala简介

Martin ordersky发明,javac的编译器,后来spark,kafka应用广发,twitter应用推广.它具备面向对象和函数式编程的特点.

我们用的是2.10.4,最新版是2.12.5

 

1. 环境安装

1)     Windous 安装jdk1.7 安装scala

2)     Linux

 

1. 第一个程序

1)     交互式编程

2)     脚本形式:通过创建文件来在eclipse/idea 中执行代码

Public class Test{

​       Public static void main(

 

java.lang._

scala._

Predef._

 

 

 

 

# 4月16日课堂笔记

## 1.HBase简介

HDFS面向批量访问模式(访问一个个文件),不是随机访问模式(访问一条条数据)

 

关系型数据库的问题:

1. 无法应对在数据规模剧增时导致的系统扩展性和性能问题(分库分表也不能很好解决,且水平扩展需要手动,费时费力)
2. 所谓关系型数据库就是我们在存储之处,就把数据结构设计好了,然后一行一行的录入数据,一旦需要修改表结构的时候,可能就需要把数据库停下来,然后才能进行更新,在web2.0时代数据结构经常发生变化.

 

HBase于传统的关系型数据库的区别主要体现在以下几个方面:

(1)   数据类型:关系型数据库采用关系模型,具有丰富的数据类型和储存方式,Hbase则采用了更加简单的数据模型,它把数据存储为未经解释的字符串.

(2)   数据操作:关系数据库中包含了丰富的操作,其中会设计复杂的多表连接.HBase操作则不存在复杂的表与表之间的关系,只有简单的插入,查询,删除,清空等,因为HBase在设计上就避免了复杂的表和表之间的关系.

(3)   存储模式:关系数据库是基于行模式存储的.

HBase是基于列存储的,每个列族都由几个文件保存,不同烈祖的文件是分离的.

行存储,是和我们看到这个表的感觉差不多的

一行一行存储的,每一行都是一个完整的数据集.

列存储:

很多学生的姓名进行存储,很多性别进行存储,很多年龄存到一起.

为什么要这么存呢?刚开始只是为了存储数据.

假如我只想看一列的数据,例如全部人的姓名,如果是按行存储,我要全部读出来.

如果是按列存储,我只要把这一列读出来就完事了,节省很多计算资源.

(4)   数据索引:关系型数据库通常可以针对不同列构建复杂的多个索引,以提高数据访问性能.HBase只要一个索引—行健,通过巧妙的设计,HBase中的所有访问方法,或者通过行健访问,或者通过行健扫描,从而使得整个系统不会慢下来.

(5)   数据维护:在关系型数据库中,更新操作会用最新的当前值去替换记录中原来的旧值,旧值被覆盖后就不会存在.而在HBase中执行更新操作时,并不会删除数据旧得版本,而是生成一个新的版本,就有的版本仍然保留.

(6)   可伸缩性: 关系型数据库很难实现横向扩展,纵向扩展的空间也比较有限

 

## 2.HBase的访问接口:

Native Java API:最常规和高效的访问方式,适合Hadoop MapReduce作业,并行批处理HBase表数据.

HBase Shell  : HBase的命令行工具,最简单的工具.适合HBase管理使用.

Thrift Gateway

Hive:简单,当需要以类似SQL语言方式来访问HBase的时候.

 

 

有了数据结构就有了基于数据结构的一系列操作,不管用什么语言核心是为了实现某些操作.

你的数据类型决定了你的操作.

所有语言都有数值型,每个数值型都封装了很多方法,例如四则运算.

生成一个RDD,可以进行一些转化和运算,

 

## 3.HBase的数据模型

#### 3.1数据模型概述

HBase是一个稀疏,多维度,排序的映射表,这张表的索引是行健,列族,列限定符和时间戳.

每个值是一个未经解释的字符串,没有数据类型.

用户在表中存储数据,每一行都有一个可排序的行键和任意多的列

表在水平方向由一个或者多个列族组成,一个列族中可以包含人一多个列,同一个列族里面的数据存储在一起.

列族支持动态扩展,可以很轻松的添加一个列族或列,无需预先定义列的数量以及类型,所有列均以字符串形式存储, 用户需要自行进行数据类型转换

 

HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留（这是和HDFS只允许追加不允许修改的特性相关的）

 

### 3.2 数据模型相关概念

表：HBase采用表来组织数据，表由行和列组成，列划分为若干个列族。避免多表连接操作，追求分析效率。

行：每个HBase表都由若干行组成，每个行由行键（row key）来标识。

列族：一个HBase表被分组成许多“列族”（Column Family）的集合，它是基本的访问控制单元，也是基本存储单元

列限定符：列族里的数据通过列限定符（或列）来定位

单元格：在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，总被视为字节数组byte[]

时间戳：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引

 

 

 

### 3.3 数据坐标

 

HBase中需要根据行键、列族、列限定符和时间戳来确定一个单元格，因此，可以视为一个“四维坐标”，即[行键, 列族, 列限定符, 时间戳]

 

### 3.4概念视图

其中冒号前面为列族名称，冒号后面是列名称，因此从某种意义上而言，HBase中大多数表都是稀疏的

 

### 3.5 物理视图

 

### 3.6 面向列的存储

直接用列的索引就能锁定列,进行分析

 

不同存储模型优劣势对比

面向行存储的数据库主要采用NSM存储模型,即一个元组(行)会被连续的存储在磁盘页中,数据是一行行进行存储,读取时也是一行行进行读取,当要选取某属性进行分析时,也需要首先扫描完整元组内容

优点:适用于联机事务性数据处理,即将分布于不同地理位置的数据利用网络进行连接,进而进行统一的存储和管理

缺点:不适合分析性操作

 

面向列存储的数据库则主要采用DSM（Decomposition Storage Model）存储模型，该模型会对关系进行垂直分解，并为每个属性分配一个子关系，每个子关系将单独存储。

优点：在批处理和即时查询等分析性操作中能够直接定位目标列，从而能够有效降低I/O开销；同一列数据类型相同，存储过程能够拥有很高的数据压缩率，从而节省存储空间

缺点：执行连接操作时要付出昂贵的元组重构代价

 

总而言之,NSM存储模型更加适合事务型应用,DSM存储模型更加适合分析性应用.

 

事务性:数据统一规范化的管理

 

## 4 HBase的实现原理

### 4.1HBase功能组件

 

主要包含三个主要功能组件:

(1)   库函数:连接到每个客户端

(2)   一个Master主服务器

(3)   许多个Region服务器

 

Master负责管理和维护HBase表的分区信息, 维护Region服务器列表, 分配Region, 负载均衡, 和NameNode功能类似

Region服务器负责存储和维护分配给自己的Region, 处理来自客户端的读写请求, 和DataNode功能类似.

客户端并不是直接从Master主服务器上读取数据, 而是在获得Region的存储位置信息后, 直接从Region服务器上读取数据.

客户端并不依赖Master, 而是通过Zookeeper来获得Region位置信息, 大多数客户端甚至从来不和Master通信, 这种设计方式使得Master负载很小

 

### 4.2表和Region

开始只有一个Region，后来不断分裂

Region拆分操作非常快（开始只是修改文件指向），接近瞬间，因为拆分之后的Region读取的仍然是原存储文件，直到“合并”过程把存储文件异步地写到独立的文件之后，才会读取新文件.

  

 

 

每个Region默认大小是100MB到200MB（2006年以前的硬件配置）

每个Region的最佳大小取决于单台服务器的有效处理能力

目前每个Region最佳大小建议1GB-2GB（2013年以后的硬件配置）

同一个Region不会被分拆到多个Region服务器（Region最小不可分）

每个Region服务器存储10-1000个Region

 

 

### 4.3 Region的定位

元数据表, 又名.META表, 储存了Region和Region服务器的映射关系

当HBase表很大时, META.表也会被分裂成多个Region

根数据表, 又名ROOT表, 记录所有元数据的具体位置

ROOT表只有唯一一个Region, 名字是在程序中被写死的

Zookeeper文件记录了ROOT表的位置

客户端访问数据时的“三级寻址”

为了加速寻址，客户端会缓存位置信息，同时，需要解决缓存失效问题

寻址过程客户端只需要询问Zookeeper服务器，不需要连接Master服务器

 

## 5  HBase运行机制

### 5.1 HBase系统架构

\1. 客户端

客户端包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程

\2. Zookeeper服务器

Zookeeper可以帮助选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题.

 

Zookeeper是一个很好的集群管理工具，被大量用于分布式计算，提供配置维护、域名服务、分布式同步、组服务等。

 

\3. Master

主服务器Master主要负责表和Region的管理工作：

管理用户对表的增加、删除、修改、查询等操作

实现不同Region服务器之间的负载均衡

在Region分裂或合并后，负责重新调整Region的分布

对发生故障失效的Region服务器上的Region进行迁移

\4. Region服务器

Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求

 

 

### 5.2 Region服务器工作原理

\1. 用户读写数据过程

用户写入数据时，被分配到相应Region服务器去执行

用户数据首先被写入到MemStore和Hlog中

只有当操作写入Hlog之后，commit()调用才会将其返回给客户端

当用户读取数据时，Region服务器会首先访问MemStore缓存，如果找不到，再去磁盘上面的StoreFile中寻找

\2. 缓存的刷新

系统会周期性地把MemStore缓存里的内容刷写到磁盘的StoreFile文件中，清空缓存，并在Hlog里面写入一个标记

每次刷写都生成一个新的StoreFile文件，因此，每个Store包含多个StoreFile文件

 

每个Region服务器都有一个自己的HLog 文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入MemStore，再刷写到StoreFile，最后删除旧的Hlog文件，开始为用户提供服务.

\3. StoreFile的合并

每次刷写都生成一个新的StoreFile，数量太多，影响查找速度

调用Store.compact()把多个合并成一个

合并操作比较耗费资源，只有数量达到一个阈值才启动合并

 

 

### 5.3 Store工作原理

Store是Region服务器的核心

多个StoreFile合并成一个

单个StoreFile过大时，又触发分裂操作，1个父Region被分裂成两个子Region

 

 

 

### 5.4 HLog工作原理

分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复

HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log）

用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘.

 

Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master

Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录

系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region服务器中，并把与该Region对象相关的HLog日志记录也发送给相应的Region服务器

Region服务器领取到分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复

共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志

 

## 6 HBase应用方案

### 6.1 HBase实际应用中的性能优化方法

**行键（Row Key）**

行键是按照字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。

举个例子：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为行键的一部分，由于是字典序排序，所以可以使用Long.MAX_VALUE - timestamp作为行键，这样能保证新写入的数据在读取时可以被快速命中。

**InMemory**

创建表的时候，可以通过HColumnDescriptor.setInMemory(true)将表放到Region服务器的缓存中，保证在读取的时候被cache命中。

**Max Version**

创建表的时候，可以通过HColumnDescriptor.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)。

Time To Live

创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 * 24 * 60 * 60)。

### 6.2 HBase性能监视

Master-status(自带)

Ganglia

OpenTSDB

Ambari

HBase Master默认基于Web的UI服务端口为60010，HBase region服务器默认基于Web的UI服务端口为60030.如果master运行在名为master.foo.com的主机中，mater的主页地址就是http://master.foo.com:60010，用户可以通过Web浏览器输入这个地址查看该页面

可以查看HBase集群的当前状态

### 6.3 在HBase之上构建SQL引擎

NoSQL区别于关系型数据库的一点就是NoSQL不使用SQL作为查询语言，至于为何在NoSQL数据存储HBase上提供SQL接口，有如下原因：

 

　　1.易使用。使用诸如SQL这样易于理解的语言，使人们能够更加轻松地使用HBase。

1. 减少编码。使用诸如SQL这样更高层次的语言来编写，减少了编写的代码量。

方案：

1.Hive整合HBase

2.Phoenix

**1.Hive整合HBase**

　　Hive与HBase的整合功能从Hive0.6.0版本已经开始出现，利用两者对外的API接口互相通信，通信主要依靠hive_hbase-handler.jar工具包(Hive Storage Handlers)。由于HBase有一次比较大的版本变动，所以并不是每个版本的Hive都能和现有的HBase版本进行整合，所以在使用过程中特别注意的就是两者版本的一致性。

**2.Phoenix**

　　Phoenix由Salesforce.com开源，是构建在Apache HBase之上的一个SQL中间层，可以让开发者在HBase上执行SQL查询。　

 

### 6.4 构建HBase二级索引

HBase只有一个针对行健的索引

访问HBase表中的行，只有三种方式：

通过单个行健访问

通过一个行健的区间来访问

全表扫描

 

使用其他产品为HBase行健提供索引功能：

Hindex二级索引

HBase+Redis

HBase+solr

 

原理：采用HBase0.92版本之后引入的Coprocessor特性

 

7.HBase操作语法

create 'student','Sname','Ssex','Sage','course'

第一行是表名, 后面的是列族.

 

# 4月17日 课堂笔记

## HBase和Hive的区别—数据库和数据仓库的区别

Hive: 数据格式:是以文本格式存储的格式

HBase: 全部是未经解释的字符串

Hive的数据结构比HBase复杂多了, 有字符型, 字符串型等.

虽然Hive不是非常好的存储工具, 但是他是一个非常好的数据仓库工具.

Hive有针对于HQL的代码.

数据库和数据仓库之间的区别.

可以用Hive进行一些数据探索和数据分析.

## HBase和Hive进行集成

 

 

## Sqoop操作方法

Import导入:

 

Export 导出:

# 4月18日课堂笔记

好像每天我上课开都在溜号, 必须得想想办法解决这个问题了, 绝对不能这样下去.

 

 